<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Articles | root0emir</title>
    <meta name="description"
        content="Technical articles by root0emir on AI/ML, Linux, software development, and system engineering.">
    <link rel="stylesheet" href="static/styles.css">
    <link rel="icon" href="Assets/root0icon.png" type="image/x-icon">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css">
</head>

<body>
    <div class="bg-grid"></div>
    <div class="bg-orb bg-orb-1"></div>
    <div class="bg-orb bg-orb-2"></div>
    <canvas id="bg-particles" class="bg-canvas"></canvas>

    <nav class="navbar" id="navbar">
        <div class="nav-inner">
            <a href="index.html" class="nav-brand">
                <div class="brand-icon"><img src="Assets/root0icon.png" alt="Killrozero"></div>Killrozero<span
                    class="accent"></span>
            </a>
            <div class="nav-links" id="nav-links">
                <a href="index.html">Home</a>
                <a href="about.html">About</a>
                <a href="distros.html">Distro's</a>
                <a href="blogs.html">Blogs</a>
                <a href="articles.html" class="active">Articles</a>
                <a href="ainews.html">AI News</a>
                <a href="contact.html">Contact</a>
            </div>
            <button class="nav-toggle" id="nav-toggle"
                aria-label="Toggle menu"><span></span><span></span><span></span></button>
        </div>
    </nav>

    <div class="page-content" style="padding-top:calc(var(--nav-height) + 40px);">
        <section class="section">
            <div class="container">
                <div class="section-header reveal">
                    <h2>Technical <span class="accent">Articles</span></h2>
                    <p>In-depth technical articles on AI/ML, Linux, and software development.</p>
                </div>

                <div class="blog-grid">

                    <!-- =============================== -->
                    <!-- AI/ML ARTICLES            -->
                    <!-- =============================== -->

                    <!-- AI Article 1: Transformer Architecture -->
                    <div class="blog-card reveal">
                        <div class="blog-card-header">
                            <h3>Transformer Architecture: Deep Dive into the Attention Mechanism</h3>
                            <span class="tag"
                                style="background:rgba(168,85,247,0.1);color:#c084fc;border-color:rgba(168,85,247,0.2);">AI/ML</span>
                        </div>
                        <p>A technical examination of the Transformer architecture and the Self-Attention mechanism that
                            forms the foundation of modern NLP.</p>
                        <div class="blog-excerpt">
                            <div class="blog-content article-content">
                                <h2>What is the Transformer</h2>
                                <p>The Transformer architecture, introduced in Google's 2017 paper "Attention Is All You
                                    Need," revolutionized the NLP field. It eliminated the sequential processing
                                    requirement of RNNs and LSTMs, enabling parallel processing of input sequences. This
                                    parallelism is not just a performance improvement but a fundamental shift in how
                                    models process language. Instead of treating text as a stream that must be read
                                    left-to-right, Transformers can look at the entire input simultaneously,
                                    understanding relationships between words regardless of their distance from each
                                    other.</p>
                                <p>Before Transformers, sequence-to-sequence models relied on encoder-decoder
                                    architectures built with recurrent networks. These had a critical bottleneck: the
                                    entire input sequence had to be compressed into a fixed-size context vector. For
                                    long sequences, information was inevitably lost. The attention mechanism solved this
                                    by allowing the decoder to look at all encoder hidden states during generation, but
                                    the recurrent nature still limited training speed.</p>
                                <h2>Self-Attention Mechanism</h2>
                                <p>Self-attention calculates the relationship of every element in a sequence with all
                                    other elements. Three vectors are produced for each token: <code>Query (Q)</code>,
                                    <code>Key (K)</code>, and <code>Value (V)</code>. These are obtained by multiplying
                                    the input embedding by learned weight matrices W_Q, W_K, and W_V. The attention
                                    score is calculated with this formula:
                                </p>
                                <pre><code>Attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) * V</code></pre>
                                <p>Here <code>d_k</code> is the dimension of the key vector and is used for scaling.
                                    Without this scaling factor, the dot products can grow very large for
                                    high-dimensional vectors, pushing the softmax function into regions where gradients
                                    are extremely small. This scaling prevents the gradient vanishing problem and
                                    ensures stable training dynamics.</p>
                                <p>Intuitively, the Q/K/V framework can be understood as: Q asks "what am I looking
                                    for", K answers "what do I contain", and V says "what information should I provide."
                                    The softmax of QK^T creates a probability distribution over all positions,
                                    determining how much attention each position should receive. This weighted sum of
                                    values creates a context-aware representation of each token.</p>
                                <h2>Multi-Head Attention</h2>
                                <p>Instead of a single attention calculation, multiple "heads" run in parallel. Each
                                    head works in a different representation subspace, allowing the model to capture
                                    different types of relationships. For example, one head might focus on syntactic
                                    relationships (subject-verb agreement), another on semantic relationships (word
                                    meaning similarity), and another on positional relationships (proximity in the
                                    sentence).</p>
                                <p>The implementation splits the d_model dimensional space into h heads, each with
                                    dimension d_k = d_model/h. All heads are computed in parallel and the results are
                                    concatenated and projected through a final linear layer. With 8 heads and
                                    d_model=512, each head operates on 64-dimensional vectors. This is computationally
                                    equivalent to single-head attention with full dimensionality but captures richer
                                    patterns.</p>
                                <h2>Positional Encoding</h2>
                                <p>Transformers by their nature do not know sequence order. Therefore sinusoidal or
                                    learnable positional encodings are added. The original paper used sinusoidal
                                    functions with different frequencies for each dimension. This approach has the
                                    theoretical advantage that the model can potentially generalize to sequence lengths
                                    longer than those seen during training, since the relative position information is
                                    encoded through trigonometric functions that extend continuously.</p>
                                <p>Modern models use more advanced methods like RoPE (Rotary Position Embedding), which
                                    encodes position by rotating the Q and K vectors, and ALiBi (Attention with Linear
                                    Biases), which adds a position-dependent penalty to the attention scores without
                                    modifying the embeddings at all. RoPE has become the standard for most modern LLMs
                                    because it naturally encodes relative position rather than absolute position, which
                                    is more useful for language understanding.</p>
                                <h2>Feed-Forward Networks and Layer Normalization</h2>
                                <p>Each Transformer layer also contains a position-wise feed-forward network (FFN) with
                                    two linear transformations and a non-linear activation (originally ReLU, now
                                    commonly SwiGLU or GeGLU). The FFN operates independently on each position, acting
                                    as the "thinking" part of the model where information from the attention output is
                                    processed and transformed. The hidden dimension of the FFN is typically 4x the model
                                    dimension.</p>
                                <p>Layer normalization is applied before each sub-layer (Pre-LN, used in most modern
                                    models) or after (Post-LN, used in the original paper). Pre-LN is preferred because
                                    it leads to more stable training, especially for deep models, and does not require
                                    the careful learning rate warmup that Post-LN demands.</p>
                                <h2>Practical Applications</h2>
                                <p>BERT (encoder-only), GPT (decoder-only), and T5 (encoder-decoder) are different
                                    variants of the Transformer architecture. Each is optimized for different tasks.
                                    BERT excels at text understanding tasks like classification, named entity
                                    recognition, and question answering through bidirectional context. GPT excels at
                                    text generation through autoregressive prediction, learning to predict the next
                                    token given all previous tokens. T5 frames all NLP tasks as text-to-text problems,
                                    making it versatile for both understanding and generation. The field has largely
                                    converged on decoder-only architectures for large language models, as scaling laws
                                    show they most efficiently convert compute into capability.</p>
                            </div>
                        </div>
                        <button class="blog-read-more"><span>Read More</span> <i
                                class="fas fa-chevron-down"></i></button>
                    </div>

                    <!-- AI Article 2: LoRA and QLoRA -->
                    <div class="blog-card reveal">
                        <div class="blog-card-header">
                            <h3>LoRA and QLoRA: Efficient Model Fine-Tuning Techniques</h3>
                            <span class="tag"
                                style="background:rgba(168,85,247,0.1);color:#c084fc;border-color:rgba(168,85,247,0.2);">AI/ML</span>
                        </div>
                        <p>How can you fine-tune large language models with limited resources. A technical look at LoRA
                            and QLoRA techniques.</p>
                        <div class="blog-excerpt">
                            <div class="blog-content article-content">
                                <h2>The Fine-Tuning Problem</h2>
                                <p>Fine-tuning large language models (LLMs) requires massive GPU memory. A 7B parameter
                                    model needs approximately 56GB VRAM for full fine-tuning (parameters + gradients +
                                    optimizer states in FP32). This is an inaccessible resource for most researchers and
                                    developers. The parameter count alone at FP16 needs 14GB, but the Adam optimizer
                                    stores two additional state values per parameter, pushing memory requirements to 4x
                                    the model size.</p>
                                <p>Full fine-tuning also risks catastrophic forgetting, where the model loses its
                                    general capabilities while specializing on the new task. This requires careful
                                    learning rate tuning, warmup schedules, and sometimes freezing early layers. The
                                    computational cost means iteration cycles are slow, making hyperparameter search
                                    expensive.</p>
                                <h2>LoRA (Low-Rank Adaptation)</h2>
                                <p>LoRA freezes the original model weights and adds small "adapter" matrices to each
                                    layer. These matrices use a low-rank decomposition:</p>
                                <pre><code>W' = W + BA
// W: original weight matrix (frozen)
// B: (d x r) matrix, r much smaller than d
// A: (r x d) matrix
// Total trained parameters: 2 * d * r</code></pre>
                                <p>The rank value (r) is typically kept between 4-64. This reduces the number of
                                    trainable parameters by over 99%. In a 7B parameter model, only around 4M parameters
                                    are trained. The key insight is that the weight updates during fine-tuning have a
                                    low intrinsic rank, meaning the useful information in the update can be captured by
                                    a low-rank approximation without significant loss.</p>
                                <p>LoRA is applied to specific weight matrices in the attention layers, usually the
                                    query and value projection matrices (Q and V). Some implementations also apply it to
                                    the key projection, output projection, or even the FFN layers. Experimentation shows
                                    that applying LoRA to more layers generally helps, but with diminishing returns. The
                                    alpha parameter controls the scale of the LoRA update relative to the original
                                    weights, functioning as an implicit learning rate modifier.</p>
                                <p>A major advantage of LoRA is that the adapter weights can be merged with the original
                                    model at inference time, introducing zero additional latency. You can also maintain
                                    multiple LoRA adapters for different tasks and swap them at runtime, effectively
                                    creating task-specific models without duplicating the base model.</p>
                                <h2>QLoRA</h2>
                                <p>QLoRA combines LoRA with 4-bit quantization. Using the NF4 (Normal Float 4-bit) data
                                    type and double quantization, it reduces memory usage even further. You can
                                    fine-tune a 65B parameter model on a single 48GB GPU. The NF4 data type is
                                    specifically designed for normally distributed weights, which neural network weights
                                    tend to be after training, providing better information density than uniform
                                    quantization.</p>
                                <p>Double quantization quantizes the quantization constants themselves, saving an
                                    additional 0.37 bits per parameter. Combined with paged optimizers that use unified
                                    memory to handle memory spikes, QLoRA makes large model fine-tuning accessible on
                                    consumer hardware. The quality loss from 4-bit quantization is minimal because the
                                    LoRA adapters are trained in full precision, compensating for any quantization
                                    artifacts.</p>
                                <h2>Practical Application</h2>
                                <p>Implementing LoRA/QLoRA with the Hugging Face PEFT library takes just a few lines of
                                    code:</p>
                                <pre><code>from peft import LoraConfig, get_peft_model
config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)
model = get_peft_model(base_model, config)</code></pre>
                                <p>Dataset preparation, hyperparameter tuning, and choosing evaluation metrics correctly
                                    are keys to success. For instruction-following fine-tuning, use the Alpaca or
                                    ShareGPT format. For domain adaptation, curate a high-quality dataset of at least a
                                    few hundred examples. Use validation loss and task-specific metrics (not just
                                    training loss) to detect overfitting, which is common with small datasets. The
                                    learning rate should typically be 2-5x higher than full fine-tuning learning rates
                                    since fewer parameters are being updated.</p>
                            </div>
                        </div>
                        <button class="blog-read-more"><span>Read More</span> <i
                                class="fas fa-chevron-down"></i></button>
                    </div>

                    <!-- AI Article 3: Anomaly Detection -->
                    <div class="blog-card reveal">
                        <div class="blog-card-header">
                            <h3>AI-Powered Anomaly Detection in Cybersecurity</h3>
                            <span class="tag"
                                style="background:rgba(168,85,247,0.1);color:#c084fc;border-color:rgba(168,85,247,0.2);">AI/ML</span>
                        </div>
                        <p>A technical examination of how machine learning algorithms are used in network traffic
                            anomaly detection.</p>
                        <div class="blog-excerpt">
                            <div class="blog-content article-content">
                                <h2>Why Anomaly Detection Matters</h2>
                                <p>Traditional signature-based detection systems (Snort, Suricata) can catch known
                                    attacks but are blind to zero-day attacks. ML-based anomaly detection learns
                                    "normal" behavior and detects deviations from it, catching even previously unseen
                                    attacks. The fundamental advantage is that you do not need to know what the attack
                                    looks like in advance, you only need to know what normal looks like.</p>
                                <p>The shift from signature-based to behavior-based detection represents a paradigm
                                    change in security. Instead of maintaining an ever-growing database of attack
                                    signatures (which is inherently reactive), anomaly detection is proactive. It can
                                    identify novel attack patterns, insider threats, and advanced persistent threats
                                    (APTs) that specifically design their behavior to avoid matching known signatures.
                                </p>
                                <h2>Algorithms Used</h2>
                                <p><strong>Isolation Forest</strong> is based on the principle that anomalies can be
                                    isolated more easily from normal data. In each decision tree, anomalies produce
                                    shorter paths because fewer splits are needed to isolate unusual data points.
                                    Training is fast and it can work with unlabeled data, making it ideal for scenarios
                                    where labeled attack data is scarce. The algorithm creates random partitions of the
                                    feature space and measures the average path length needed to isolate each point.
                                    Normal data points, being more common, require more partitions to isolate.</p>
                                <p><strong>Autoencoder</strong> learns to compress and reconstruct normal traffic.
                                    Anomalous traffic produces high reconstruction error because the model has never
                                    learned to encode those patterns. Deep learning based, so it can capture complex
                                    patterns and non-linear relationships that simpler models miss. Variational
                                    autoencoders add a probabilistic element, enabling uncertainty estimation, which is
                                    valuable for distinguishing between genuine anomalies and noise. The reconstruction
                                    error threshold must be carefully tuned to balance detection rate against false
                                    positive rate.</p>
                                <p><strong>LSTM-based detection</strong> is ideal for detecting sequential anomalies in
                                    time series data (network traffic, log streams). It predicts the next step of a
                                    sequence, and high prediction error is flagged as anomaly. The temporal context
                                    understanding is critical for detecting slow-and-low attacks that spread malicious
                                    activity across time to avoid burst-based detection. Bidirectional LSTMs can look at
                                    both past and future context for even better detection accuracy.</p>
                                <p><strong>Graph Neural Networks (GNNs)</strong> are emerging as a powerful approach for
                                    network security. They model the network as a graph where nodes are hosts and edges
                                    are connections. This allows detection of anomalous communication patterns, lateral
                                    movement, and unusual relationship formation that point-based methods would miss
                                    entirely.</p>
                                <h2>Feature Engineering</h2>
                                <p>Rather than feeding raw network packets directly to the model, meaningful features
                                    need to be extracted: packet size statistics (mean, std, min, max, entropy), flow
                                    duration, port usage distribution, protocol ratios, connection frequency,
                                    inter-arrival times, byte ratio (sent vs received), and TCP flag distributions.
                                    Netflow data is very useful for this purpose as it provides pre-aggregated
                                    flow-level statistics.</p>
                                <p>Feature selection is equally important. Too many features lead to the curse of
                                    dimensionality, where the model has trouble distinguishing signal from noise.
                                    Techniques like mutual information, principal component analysis, and recursive
                                    feature elimination help identify the most discriminative features. Domain expertise
                                    is irreplaceable here, understanding which network behavior patterns are meaningful
                                    is as important as the algorithms themselves.</p>
                                <h2>Challenges</h2>
                                <p>Keeping the false positive rate low is the biggest challenge. Sending hundreds of
                                    false alarms per minute to a SIEM exhausts analysts and causes real threats to be
                                    overlooked, a phenomenon known as alert fatigue. Threshold optimization and ensemble
                                    methods can mitigate this problem. Multi-stage detection pipelines, where a fast
                                    initial model flags candidates and a more accurate but slower model validates them,
                                    can dramatically reduce false positives while maintaining detection coverage.</p>
                                <p>Concept drift is another major challenge. Network behavior changes over time as new
                                    applications are deployed, user patterns shift, and infrastructure evolves. Models
                                    must be regularly retrained or use online learning approaches to adapt. Without
                                    periodic retraining, detection accuracy degrades and false positive rates increase
                                    as the model's understanding of "normal" becomes outdated.</p>
                            </div>
                        </div>
                        <button class="blog-read-more"><span>Read More</span> <i
                                class="fas fa-chevron-down"></i></button>
                    </div>

                    <!-- AI Article 4: RAG Architecture -->
                    <div class="blog-card reveal">
                        <div class="blog-card-header">
                            <h3>RAG (Retrieval-Augmented Generation) Architecture</h3>
                            <span class="tag"
                                style="background:rgba(168,85,247,0.1);color:#c084fc;border-color:rgba(168,85,247,0.2);">AI/ML</span>
                        </div>
                        <p>Technical details and production best-practices of the RAG architecture that augments LLMs
                            with external knowledge.</p>
                        <div class="blog-excerpt">
                            <div class="blog-content article-content">
                                <h2>What is RAG</h2>
                                <p>RAG enables an LLM to retrieve relevant documents from a database and use them as
                                    context before generating a response. This reduces hallucination, allows the use of
                                    current information, and produces responses enriched with domain-specific knowledge.
                                    Without RAG, LLMs are limited to the knowledge encoded in their parameters during
                                    training, which becomes stale the moment training ends. RAG bridges this gap by
                                    giving the model access to a living knowledge base.</p>
                                <h2>Core Components</h2>
                                <p><strong>Document Processing:</strong> Documents are split into chunks (typically
                                    256-512 tokens). Chunk overlap (10-20%) prevents context loss at boundaries.
                                    Recursive text splitting or semantic chunking can be used. Recursive splitting
                                    respects document structure (paragraphs, sentences) while semantic chunking uses
                                    embedding similarity to find natural topic boundaries. The chunk size directly
                                    affects retrieval quality: too small and context is lost, too large and retrieval
                                    becomes imprecise.</p>
                                <p><strong>Embedding Model:</strong> Each chunk is converted to a vector representation.
                                    OpenAI ada-002, sentence-transformers, or BGE models are commonly used. Embedding
                                    dimension and model choice directly affect search quality. For domain-specific
                                    applications, fine-tuned embedding models significantly outperform generic ones. The
                                    embedding model should be evaluated on your specific data using metrics like
                                    recall@k and mean reciprocal rank.</p>
                                <p><strong>Vector Database:</strong> Stored in databases like Pinecone, Weaviate,
                                    Chroma, Milvus, or FAISS. ANN (Approximate Nearest Neighbor) algorithms enable fast
                                    similarity search. HNSW (Hierarchical Navigable Small World) graphs are the most
                                    popular ANN algorithm due to their excellent speed-accuracy tradeoff. For production
                                    systems, considerations include index update latency, filtered search support,
                                    metadata storage, and horizontal scalability.</p>
                                <h2>Advanced Techniques</h2>
                                <p><strong>Hybrid Search:</strong> Combining dense (semantic) and sparse (BM25 keyword)
                                    search for better results. Dense search captures semantic meaning while sparse
                                    search handles exact keyword matches. Reciprocal Rank Fusion or learned weighting
                                    combines the two ranked lists. This is particularly important for queries containing
                                    domain-specific terminology that the embedding model may not have seen in training.
                                </p>
                                <p><strong>Re-ranking:</strong> Re-ordering initial results with a cross-encoder. Unlike
                                    the bi-encoder used for initial retrieval, a cross-encoder processes the query and
                                    document together, enabling deeper interaction modeling. This is computationally
                                    expensive but dramatically improves precision for the top results that actually get
                                    passed to the LLM.</p>
                                <p><strong>Query Expansion:</strong> Expanding the user query with the LLM for better
                                    matching. The LLM generates a hypothetical answer document (HyDE), which is then
                                    used as the search query. This bridges the vocabulary gap between how users ask
                                    questions and how information is stored in documents.</p>
                                <p><strong>Multi-Query RAG:</strong> Rewriting the query from different perspectives for
                                    multiple searches. The original query is decomposed into sub-questions or rephrased
                                    multiple ways, each generating its own set of retrieved documents. This increases
                                    recall and ensures important context is not missed due to the phrasing of a single
                                    query.</p>
                                <p><strong>Contextual Compression:</strong> Retrieved chunks often contain irrelevant
                                    information alongside relevant content. A compression step uses an LLM to extract
                                    only the relevant portions of each chunk before passing them to the generation
                                    model. This reduces the context window usage and focuses the model's attention on
                                    the most pertinent information.</p>
                            </div>
                        </div>
                        <button class="blog-read-more"><span>Read More</span> <i
                                class="fas fa-chevron-down"></i></button>
                    </div>

                    <!-- AI Article 5: Adversarial ML -->
                    <div class="blog-card reveal">
                        <div class="blog-card-header">
                            <h3>Adversarial Machine Learning: Attacks Against ML Models</h3>
                            <span class="tag"
                                style="background:rgba(168,85,247,0.1);color:#c084fc;border-color:rgba(168,85,247,0.2);">AI/ML</span>
                        </div>
                        <p>A technical analysis of evasion, poisoning, and extraction attacks against ML models, along
                            with defense strategies.</p>
                        <div class="blog-excerpt">
                            <div class="blog-content article-content">
                                <h2>Evasion Attacks</h2>
                                <p>Adding small perturbations to input data that are imperceptible to humans but cause
                                    the model to misclassify. FGSM (Fast Gradient Sign Method) and PGD (Projected
                                    Gradient Descent) are the best-known methods. FGSM is a single-step attack that
                                    follows the gradient direction, while PGD iterates multiple steps with projection
                                    back into the allowed perturbation ball, creating stronger adversarial examples.</p>
                                <pre><code># FGSM Attack
x_adv = x + epsilon * sign(gradient_x(L(theta, x, y)))

# PGD Attack (iterative)
for i in range(steps):
    x_adv = x_adv + alpha * sign(gradient_x(L(theta, x_adv, y)))
    x_adv = clip(x_adv, x - epsilon, x + epsilon)</code></pre>
                                <p>In the cybersecurity context, evasion attacks can modify malware samples to bypass
                                    ML-based detection. By adding benign features or slightly modifying binary
                                    structure, an attacker can cause a classifier to label malicious software as benign.
                                    This has been demonstrated against commercial antivirus products that use ML for
                                    detection. The challenge is that these perturbations must preserve the malware's
                                    functionality, which constrains the attack but does not eliminate it.</p>
                                <h2>Data Poisoning</h2>
                                <p>Adding malicious examples to training data to change the model's behavior. In
                                    backdoor attacks, the model produces the desired output when a specific trigger
                                    pattern is added. Supply chain attacks through distributed pre-trained models pose a
                                    particularly high risk. A poisoned model may behave normally on standard benchmarks
                                    but contains a hidden backdoor that activates only when specific input patterns are
                                    present.</p>
                                <p>Clean-label poisoning is an especially insidious variant where the poisoned examples
                                    have correct labels but contain subtle features that shift the decision boundary.
                                    This makes detection through data inspection very difficult since each individual
                                    example looks legitimate. Defense requires analyzing the training data distribution
                                    and looking for statistical anomalies.</p>
                                <h2>Model Extraction</h2>
                                <p>Training a "shadow model" that mimics the target model's behavior through API
                                    queries. With enough queries, the model architecture and weights can be
                                    approximately copied. This is both an intellectual property concern and a security
                                    issue, as the extracted model can then be used offline to craft adversarial examples
                                    or understand the model's blind spots.</p>
                                <p>Recent research has shown that even models behind rate-limited APIs can be extracted
                                    with surprisingly few queries using active learning strategies that intelligently
                                    select the most informative queries. Watermarking techniques can help detect
                                    extracted models, and differential privacy during training can limit what an
                                    attacker can learn from query responses.</p>
                                <h2>Defense Strategies</h2>
                                <p><strong>Adversarial Training:</strong> Adding adversarial examples during training.
                                    This is the most effective known defense but increases training cost by 3-10x and
                                    can reduce clean accuracy. The model learns to be robust to perturbations within a
                                    specified epsilon ball. Min-max optimization alternates between generating
                                    adversarial examples (maximizing loss) and updating model weights (minimizing loss
                                    on adversarial examples).</p>
                                <p><strong>Certified Defenses:</strong> Guaranteeing correct prediction within a certain
                                    perturbation bound. Randomized smoothing creates a provably robust classifier by
                                    classifying the most likely class under Gaussian noise perturbations. While these
                                    provide mathematical guarantees, the certified radii are often small, limiting
                                    practical utility against strong attacks.</p>
                                <p><strong>Input Preprocessing:</strong> JPEG compression, spatial smoothing, and
                                    bit-depth reduction can remove perturbations. These defenses are easy to implement
                                    but can be defeated by adaptive attacks that account for the preprocessing in their
                                    optimization. They are best used as one layer in a defense-in-depth approach.</p>
                            </div>
                        </div>
                        <button class="blog-read-more"><span>Read More</span> <i
                                class="fas fa-chevron-down"></i></button>
                    </div>

                    <!-- AI Article 6: LLM Inference Optimization -->
                    <div class="blog-card reveal">
                        <div class="blog-card-header">
                            <h3>LLM Inference Optimization: KV-Cache, Quantization and Speculative Decoding</h3>
                            <span class="tag"
                                style="background:rgba(168,85,247,0.1);color:#c084fc;border-color:rgba(168,85,247,0.2);">AI/ML</span>
                        </div>
                        <p>Technical strategies for making large language model inference faster and more
                            memory-efficient in production.</p>
                        <div class="blog-excerpt">
                            <div class="blog-content article-content">
                                <h2>The Inference Bottleneck</h2>
                                <p>LLM inference is fundamentally memory-bandwidth bound, not compute-bound. During
                                    autoregressive generation, each token requires reading the entire model weights from
                                    memory. For a 70B model in FP16, this means reading 140GB of data per token. At
                                    typical GPU memory bandwidth of 2TB/s, the theoretical maximum is about 14 tokens
                                    per second per GPU, regardless of FLOPS.</p>
                                <h2>KV-Cache Management</h2>
                                <p>During generation, the Key and Value tensors from all previous tokens must be stored.
                                    For a 7B model generating 4096 tokens, the KV-cache alone can reach 16GB. Techniques
                                    to manage this include: <strong>PagedAttention</strong> (used in vLLM) which manages
                                    KV-cache memory like virtual memory pages, eliminating fragmentation and enabling up
                                    to 24x more requests per GPU. <strong>Multi-Query Attention (MQA)</strong> and
                                    <strong>Grouped-Query Attention (GQA)</strong> share K and V heads across multiple Q
                                    heads, reducing KV-cache size by 4-8x with minimal quality loss.
                                </p>
                                <h2>Quantization for Inference</h2>
                                <p>Post-training quantization reduces model size and increases throughput. GPTQ
                                    quantizes weights to 4-bit using second-order optimization. AWQ (Activation-aware
                                    Weight Quantization) identifies salient weight channels and preserves them at higher
                                    precision. GGUF format (used by llama.cpp) enables CPU inference with various
                                    quantization levels from Q2_K to Q8_0. The quality-size tradeoff depends on the
                                    specific model and task. For most applications, Q4_K_M provides an excellent
                                    balance.</p>
                                <h2>Speculative Decoding</h2>
                                <p>Uses a small, fast "draft" model to generate candidate token sequences, then verifies
                                    them in parallel with the large model. Since verification is parallelizable (unlike
                                    generation), this can achieve 2-3x speedups without any quality loss. The acceptance
                                    rate depends on how well the draft model matches the target model. Medusa and
                                    Lookahead decoding are variations that eliminate the need for a separate draft model
                                    by adding lightweight prediction heads to the main model.</p>
                                <h2>Batching Strategies</h2>
                                <p>Continuous batching (used in TGI, vLLM) dynamically adds and removes requests from
                                    the batch as they complete, maximizing GPU utilization. Compared to static batching,
                                    this can improve throughput by 10-20x in real-world serving scenarios with
                                    variable-length inputs and outputs. Prefill-decode disaggregation separates the
                                    compute-intensive prefill phase from the memory-intensive decode phase onto
                                    different hardware, optimizing resource usage.</p>
                            </div>
                        </div>
                        <button class="blog-read-more"><span>Read More</span> <i
                                class="fas fa-chevron-down"></i></button>
                    </div>

                    <!-- AI Article 7: Diffusion Models -->
                    <div class="blog-card reveal">
                        <div class="blog-card-header">
                            <h3>Diffusion Models: How AI Image Generation Works</h3>
                            <span class="tag"
                                style="background:rgba(168,85,247,0.1);color:#c084fc;border-color:rgba(168,85,247,0.2);">AI/ML</span>
                        </div>
                        <p>The mathematics and architecture behind Stable Diffusion, DALL-E, and other image generation
                            models.</p>
                        <div class="blog-excerpt">
                            <div class="blog-content article-content">
                                <h2>The Forward and Reverse Process</h2>
                                <p>Diffusion models work by learning to reverse a gradual noising process. In the
                                    forward process, Gaussian noise is progressively added to an image over T timesteps
                                    until it becomes pure noise. The reverse process learns to denoise, step by step,
                                    recovering the original image from noise. The model is trained to predict the noise
                                    that was added at each step, using a modified U-Net architecture.</p>
                                <h2>Latent Diffusion</h2>
                                <p>Stable Diffusion operates in latent space rather than pixel space, using a VAE
                                    (Variational Autoencoder) to compress images to 1/48th of their original size. This
                                    dramatically reduces computational requirements. The diffusion process happens in
                                    this compressed latent space, and the VAE decoder converts the result back to pixel
                                    space. This architectural choice is what made high-quality image generation feasible
                                    on consumer GPUs.</p>
                                <h2>Conditioning with CLIP</h2>
                                <p>Text-to-image generation uses CLIP (Contrastive Language-Image Pre-training) to
                                    encode text prompts into embeddings that guide the denoising process.
                                    Cross-attention layers in the U-Net allow the model to attend to different parts of
                                    the text prompt at different spatial locations. Classifier-free guidance (CFG)
                                    strengthens the text conditioning by interpolating between conditional and
                                    unconditional predictions, with the guidance scale controlling the tradeoff between
                                    quality and diversity.</p>
                                <h2>ControlNet and Fine-Tuning</h2>
                                <p>ControlNet adds spatial conditioning to diffusion models, enabling control through
                                    edge maps, depth maps, pose skeletons, and more. It creates a trainable copy of the
                                    encoder blocks and connects them to the original model through zero convolutions,
                                    allowing fine-grained control while preserving the base model's quality. DreamBooth
                                    and Textual Inversion allow personalization with just a few images, teaching the
                                    model new concepts or styles.</p>
                            </div>
                        </div>
                        <button class="blog-read-more"><span>Read More</span> <i
                                class="fas fa-chevron-down"></i></button>
                    </div>

                    <!-- =============================== -->
                    <!-- SECURITY ARTICLES (30%)          -->
                    <!-- =============================== -->

                    <!-- Security Article 1: Kernel Hardening -->
                    <div class="blog-card reveal">
                        <div class="blog-card-header">
                            <h3>Linux Kernel Hardening: Comprehensive Guide</h3>
                            <span class="tag security">Security</span>
                        </div>
                        <p>Sysctl parameters, kernel module management, and compile-time options for hardening the Linux
                            kernel against attacks.</p>
                        <div class="blog-excerpt">
                            <div class="blog-content article-content">
                                <h2>Sysctl Hardening</h2>
                                <p>Kernel parameters can be set in <code>/etc/sysctl.conf</code> or the
                                    <code>/etc/sysctl.d/</code> directory. Critical settings:
                                </p>
                                <pre><code># Network hardening
net.ipv4.conf.all.rp_filter = 1
net.ipv4.conf.all.accept_redirects = 0
net.ipv4.conf.all.send_redirects = 0
net.ipv4.conf.all.accept_source_route = 0
net.ipv4.tcp_syncookies = 1
net.ipv4.icmp_echo_ignore_broadcasts = 1
net.ipv6.conf.all.accept_ra = 0

# Kernel hardening
kernel.randomize_va_space = 2  # Full ASLR
kernel.kptr_restrict = 2      # Kernel pointer restriction
kernel.dmesg_restrict = 1     # Restrict dmesg
kernel.yama.ptrace_scope = 2  # Restrict ptrace
kernel.unprivileged_bpf_disabled = 1
kernel.kexec_load_disabled = 1
kernel.sysrq = 0              # Disable SysRq</code></pre>
                                <p>Each of these settings addresses a specific attack vector. <code>rp_filter</code>
                                    enables reverse path filtering, preventing IP spoofing attacks. Disabling ICMP
                                    redirects prevents route manipulation attacks. <code>tcp_syncookies</code> protects
                                    against SYN flood attacks by using cryptographic cookies instead of allocating
                                    memory for half-open connections. Full ASLR randomizes the memory layout of
                                    processes, making exploitation significantly harder.</p>
                                <p><code>kptr_restrict=2</code> prevents kernel pointer leaks that attackers use to
                                    defeat KASLR. <code>ptrace_scope=2</code> restricts debugging capabilities that
                                    malware uses for code injection. Disabling unprivileged BPF prevents local privilege
                                    escalation through BPF vulnerabilities, which have been a frequent source of kernel
                                    exploits in recent years.</p>
                                <h2>Kernel Module Management</h2>
                                <p>Blacklist unused kernel modules. Especially disable USB storage, firewire, and
                                    thunderbolt, which are physical attack vectors:</p>
                                <pre><code># /etc/modprobe.d/blacklist.conf
install usb-storage /bin/false
install firewire-core /bin/false
install thunderbolt /bin/false
install cramfs /bin/false
install freevxfs /bin/false
install hfs /bin/false
install hfsplus /bin/false
install udf /bin/false</code></pre>
                                <p>These filesystem modules are rarely needed on servers and represent unnecessary
                                    attack surface. Firewire and Thunderbolt devices can perform DMA attacks that bypass
                                    all software security. On high-security systems, consider using
                                    <code>kernel.modules_disabled=1</code> after boot to prevent any new module loading,
                                    though this requires careful testing to ensure all needed modules are loaded during
                                    boot.
                                </p>
                                <h2>Boot Security</h2>
                                <p>Encrypt GRUB, enable Secure Boot. Add to kernel command line:
                                    <code>init_on_alloc=1 init_on_free=1 slab_nomerge lockdown=confidentiality page_alloc.shuffle=1</code>.
                                    These parameters increase memory safety and prevent the kernel from being modified
                                    at runtime. <code>init_on_alloc</code> and <code>init_on_free</code> zero memory on
                                    allocation and deallocation, preventing information leaks through uninitialized
                                    memory. <code>slab_nomerge</code> prevents slab cache merging, which is exploited in
                                    use-after-free attacks. <code>lockdown=confidentiality</code> prevents even root
                                    from accessing kernel memory, loading unsigned modules, or performing other
                                    operations that could compromise kernel integrity.
                                </p>
                            </div>
                        </div>
                        <button class="blog-read-more"><span>Read More</span> <i
                                class="fas fa-chevron-down"></i></button>
                    </div>

                    <!-- Security Article 2: Firewall Design -->
                    <div class="blog-card reveal">
                        <div class="blog-card-header">
                            <h3>Firewall Design with IPtables and Nftables</h3>
                            <span class="tag security">Security</span>
                        </div>
                        <p>How to design an effective firewall on Linux. IPtables chains, nftables syntax, and best
                            practices.</p>
                        <div class="blog-excerpt">
                            <div class="blog-content article-content">
                                <h2>Basic IPtables Architecture</h2>
                                <p>IPtables consists of three main chains: INPUT (incoming traffic), OUTPUT (outgoing
                                    traffic), and FORWARD (forwarded traffic). Rules in each chain are processed
                                    top-to-bottom, and the first matching rule determines the action. This ordering is
                                    important for both security and performance.</p>
                                <pre><code># Default DROP policy
iptables -P INPUT DROP
iptables -P FORWARD DROP
iptables -P OUTPUT ACCEPT

# Loopback allow
iptables -A INPUT -i lo -j ACCEPT

# Established connections
iptables -A INPUT -m conntrack --ctstate ESTABLISHED,RELATED -j ACCEPT

# SSH with rate limiting
iptables -A INPUT -p tcp --dport 22 -m conntrack --ctstate NEW \
  -m recent --set --name SSH
iptables -A INPUT -p tcp --dport 22 -m conntrack --ctstate NEW \
  -m recent --update --seconds 60 --hitcount 4 --name SSH -j DROP
iptables -A INPUT -p tcp --dport 22 -j ACCEPT

# Port scan protection
iptables -A INPUT -p tcp --tcp-flags ALL NONE -j DROP
iptables -A INPUT -p tcp --tcp-flags ALL ALL -j DROP
iptables -A INPUT -p tcp ! --syn -m conntrack --ctstate NEW -j DROP

# ICMP rate limiting
iptables -A INPUT -p icmp --icmp-type echo-request \
  -m limit --limit 1/s --limit-burst 4 -j ACCEPT
iptables -A INPUT -p icmp --icmp-type echo-request -j DROP</code></pre>
                                <p>The rate limiting configuration for SSH allows a maximum of 3 new connections per
                                    minute from any single IP. The fourth attempt within 60 seconds triggers a DROP.
                                    This effectively neutralizes brute-force attacks while allowing legitimate use. The
                                    ICMP rate limiting prevents ping floods while still allowing basic connectivity
                                    testing.</p>
                                <h2>Migration to Nftables</h2>
                                <p>Nftables is the modern successor to iptables. It offers cleaner syntax, better
                                    performance through fewer kernel context switches, and support for IPv4, IPv6, and
                                    ARP under a single framework. Debian 10+ and RHEL 8+ use nftables by default. The
                                    <code>iptables-translate</code> tool helps migrate existing rules. Key advantages
                                    include sets (for efficient IP list matching), maps, concatenated matches, and
                                    stateful objects.
                                </p>
                                <h2>Defense in Depth</h2>
                                <p>A firewall alone is not sufficient. Fail2ban for automatic banning watches log files
                                    and dynamically adds firewall rules for offending IPs. GeoIP filtering through the
                                    xt_geoip module blocks traffic from specific countries. IDS/IPS (Suricata) for deep
                                    packet inspection catches application-layer attacks that firewalls cannot see.
                                    Connection tracking with timeouts prevents resource exhaustion from abandoned
                                    connections. Logging suspicious traffic to a central SIEM enables correlation
                                    analysis across multiple systems.</p>
                            </div>
                        </div>
                        <button class="blog-read-more"><span>Read More</span> <i
                                class="fas fa-chevron-down"></i></button>
                    </div>

                    <!-- Security Article 3: SOC Operations -->
                    <div class="blog-card reveal">
                        <div class="blog-card-header">
                            <h3>Blue Team SOC Operations: SIEM, Log Analysis and Incident Response</h3>
                            <span class="tag security">Security</span>
                        </div>
                        <p>SIEM configuration, log correlation, threat hunting, and incident response processes for SOC
                            analysts.</p>
                        <div class="blog-excerpt">
                            <div class="blog-content article-content">
                                <h2>SIEM Architecture</h2>
                                <p>Centralized log collection and analysis with ELK Stack (Elasticsearch, Logstash,
                                    Kibana) or Splunk. Log sources include: firewall, IDS/IPS, endpoint security,
                                    application logs, authentication logs, DNS query logs, and proxy logs. Logstash
                                    pipelines handle parsing, enrichment, and normalization. Each log source uses a
                                    different format and needs a specific parser. Grok patterns or custom JSON parsers
                                    transform raw logs into structured, searchable data. Enrichment adds context like
                                    GeoIP, threat intelligence lookups, and asset information.</p>
                                <p>Sizing is critical for SIEM deployment. Calculate expected events per second (EPS),
                                    storage requirements (including retention period), and search performance needs. A
                                    typical enterprise generates 5,000-50,000 EPS. At 500 bytes per event and 30 days
                                    retention, that is 6.5TB to 65TB of storage. Hot-warm-cold tiering and index
                                    lifecycle management keep costs manageable.</p>
                                <h2>Correlation Rules</h2>
                                <p>Logs that seem meaningless individually reveal attacks when combined. Examples: 10+
                                    failed SSH logins from the same IP within 5 minutes indicates brute force.
                                    Simultaneous logins from the same user in different geographies indicates credential
                                    theft. Unusual DNS queries from an endpoint indicates C2 beaconing. A sequence of
                                    reconnaissance, credential access, and lateral movement events from the same source
                                    indicates an APT. Correlation rules should align with the MITRE ATT&CK framework to
                                    ensure coverage of known techniques.</p>
                                <h2>Threat Hunting</h2>
                                <p>Use the MITRE ATT&CK framework for proactive threat hunting. Create search queries
                                    for each technique. Collect real-time data from endpoints with OSQuery. Write
                                    platform-independent detection rules with Sigma rules. Hypothesis-driven hunting
                                    starts with a specific theory (e.g., "there may be PowerShell-based lateral
                                    movement") and searches for evidence. Data-driven hunting uses statistical analysis
                                    to find outliers without a specific hypothesis. Stack counting, long-tail analysis,
                                    and frequency analysis are key techniques.</p>
                                <h2>Incident Response</h2>
                                <p>NIST IR Framework: Preparation, Detection and Analysis, Containment, Eradication,
                                    Recovery, Lessons Learned. Every phase must be documented and timestamped. During
                                    containment, network segmentation, account deactivation, and forensic image creation
                                    are critical. Evidence preservation follows the chain of custody requirements.
                                    Memory acquisition should happen before disk imaging since volatile data is lost on
                                    shutdown. Document every action taken, every system touched, and every piece of
                                    evidence collected. Post-incident review should produce actionable improvements to
                                    detection rules, response procedures, and infrastructure security.</p>
                            </div>
                        </div>
                        <button class="blog-read-more"><span>Read More</span> <i
                                class="fas fa-chevron-down"></i></button>
                    </div>

                    <!-- Security Article 4: AppArmor & SELinux -->
                    <div class="blog-card reveal">
                        <div class="blog-card-header">
                            <h3>Mandatory Access Control: AppArmor vs SELinux Deep Dive</h3>
                            <span class="tag security">Security</span>
                        </div>
                        <p>A comprehensive comparison of AppArmor and SELinux, with practical profile writing and policy
                            management.</p>
                        <div class="blog-excerpt">
                            <div class="blog-content article-content">
                                <h2>MAC vs DAC</h2>
                                <p>Traditional Linux permissions (DAC - Discretionary Access Control) rely on file
                                    ownership and permission bits. The fundamental flaw is that any process running as
                                    root bypasses all DAC checks. Mandatory Access Control adds a layer of security
                                    policies enforced by the kernel that even root cannot override. If a compromised
                                    root process is confined by MAC policy, it cannot access resources outside its
                                    allowed scope.</p>
                                <h2>AppArmor</h2>
                                <p>AppArmor uses path-based access control, which means policies reference file paths
                                    directly. This makes profiles human-readable and relatively easy to write. Profile
                                    modes: <strong>enforce</strong> (denies violations) and <strong>complain</strong>
                                    (logs violations without denying). The aa-genprof tool can generate profiles by
                                    monitoring a program's behavior during normal operation, creating an initial policy
                                    that can then be refined.</p>
                                <pre><code># Example AppArmor profile for nginx
/usr/sbin/nginx {
  /etc/nginx/** r,
  /var/log/nginx/** w,
  /var/www/** r,
  /run/nginx.pid rw,
  network inet tcp,
  capability net_bind_service,
  deny /etc/shadow r,
  deny /home/** rwx,
}</code></pre>
                                <h2>SELinux</h2>
                                <p>SELinux uses label-based access control. Every file, process, and network port has a
                                    security context (label). Policies define which labels can interact with which other
                                    labels. This is more granular than AppArmor but significantly more complex. The Type
                                    Enforcement model assigns types to objects and domains to subjects, with policy
                                    rules governing all interactions. SELinux also supports Multi-Level Security (MLS)
                                    for environments requiring data classification.</p>
                                <h2>Which to Choose</h2>
                                <p>AppArmor: easier to learn, sufficient for most use cases, default in
                                    Debian/Ubuntu/SUSE. SELinux: more granular, better for high-security environments,
                                    default in RHEL/CentOS/Fedora. Both are significantly better than no MAC at all. The
                                    best choice depends on your distribution, team expertise, and security requirements.
                                    For a hardened distribution like Slarpx, I use AppArmor with custom profiles for
                                    every network-facing service.</p>
                            </div>
                        </div>
                        <button class="blog-read-more"><span>Read More</span> <i
                                class="fas fa-chevron-down"></i></button>
                    </div>

                    <!-- =============================== -->
                    <!-- WEB/MOBILE DEV ARTICLES (20%)    -->
                    <!-- =============================== -->

                    <!-- Web Article 1: OWASP -->
                    <div class="blog-card reveal">
                        <div class="blog-card-header">
                            <h3>Modern Web Application Security: OWASP Top 10 and Beyond</h3>
                            <span class="tag"
                                style="background:rgba(34,197,94,0.1);color:#4ade80;border-color:rgba(34,197,94,0.2);">Web</span>
                        </div>
                        <p>Defense techniques against OWASP Top 10 vulnerabilities and secure coding principles for
                            protecting your web applications.</p>
                        <div class="blog-excerpt">
                            <div class="blog-content article-content">
                                <h2>Injection Attacks</h2>
                                <p>SQL Injection, NoSQL Injection, OS Command Injection all share the same fundamental
                                    problem: user input being processed without validation. The solution: parameterized
                                    queries, prepared statements, and input validation. Never construct queries by
                                    concatenating user input. Use ORM frameworks that handle parameterization
                                    automatically. For command injection, avoid shell execution entirely and use library
                                    functions instead.</p>
                                <pre><code>// Bad - open to SQL Injection
query = "SELECT * FROM users WHERE id = " + userId;

// Good - parameterized query
query = "SELECT * FROM users WHERE id = ?";
stmt.setString(1, userId);</code></pre>
                                <h2>XSS (Cross-Site Scripting)</h2>
                                <p>There are Stored, Reflected, and DOM-based XSS types. Defense: output encoding
                                    (context-specific for HTML, JS, URL, CSS), Content-Security-Policy (CSP) headers
                                    that restrict script sources, HttpOnly and Secure cookie flags. Modern frameworks
                                    like React and Vue automatically escape output by default, but developers must be
                                    careful with dangerouslySetInnerHTML and v-html. Trusted Types API provides an
                                    additional layer of protection by requiring sanitization before DOM injection
                                    points.</p>
                                <h2>Authentication and Session Management</h2>
                                <p>Password hashing with bcrypt or argon2id (never MD5 or SHA-256 alone), JWT token
                                    expiration with short-lived access tokens (15-30 minutes) and longer refresh tokens
                                    (7-30 days), refresh token rotation that invalidates old refresh tokens on use, and
                                    MFA implementation. Use SameSite cookie attribute for CSRF protection. Implement
                                    account lockout or progressive delays after failed attempts. Store sessions
                                    server-side for sensitive applications rather than relying solely on JWTs.</p>
                                <h2>Security Headers</h2>
                                <pre><code>Content-Security-Policy: default-src 'self'; script-src 'self'
X-Content-Type-Options: nosniff
X-Frame-Options: DENY
Strict-Transport-Security: max-age=31536000; includeSubDomains
Referrer-Policy: strict-origin-when-cross-origin
Permissions-Policy: geolocation=(), camera=()
Cross-Origin-Opener-Policy: same-origin
Cross-Origin-Resource-Policy: same-origin</code></pre>
                                <p>Each header addresses a specific attack class. CSP prevents XSS and data injection.
                                    HSTS prevents protocol downgrade and cookie hijacking. X-Frame-Options prevents
                                    clickjacking. COOP prevents Spectre-like side-channel attacks. Test your headers
                                    with securityheaders.com and observatory.mozilla.org.</p>
                            </div>
                        </div>
                        <button class="blog-read-more"><span>Read More</span> <i
                                class="fas fa-chevron-down"></i></button>
                    </div>

                    <!-- Web Article 2: PWA -->
                    <div class="blog-card reveal">
                        <div class="blog-card-header">
                            <h3>Progressive Web Apps: Bringing Mobile Experience to the Web</h3>
                            <span class="tag"
                                style="background:rgba(34,197,94,0.1);color:#4ade80;border-color:rgba(34,197,94,0.2);">Mobile</span>
                        </div>
                        <p>Creating native-like mobile experiences with PWAs: Service Workers, Web App Manifest,
                            offline-first approach, and push notifications.</p>
                        <div class="blog-excerpt">
                            <div class="blog-content article-content">
                                <h2>What is a PWA</h2>
                                <p>A Progressive Web App is a web application built with web technologies (HTML, CSS,
                                    JS) that behaves like a native application. It can work offline, be installed on the
                                    device (Add to Home Screen), send push notifications, and access device APIs. PWAs
                                    bridge the gap between the reach of the web and the capability of native apps. They
                                    are indexed by search engines, work across platforms, and do not require app store
                                    distribution.</p>
                                <h2>Service Workers</h2>
                                <p>A Service Worker is a proxy that sits between the browser and the network. It can
                                    intercept network requests, serve responses from cache, and run in the background.
                                    This enables the offline-first experience. Service Workers have a lifecycle
                                    (install, activate, fetch) that must be carefully managed. Cache versioning ensures
                                    users get updated content.</p>
                                <pre><code>// Service Worker with cache versioning
const CACHE_NAME = 'v2';
const ASSETS = ['/', '/index.html', '/styles.css', '/app.js'];

self.addEventListener('install', event => {
  event.waitUntil(
    caches.open(CACHE_NAME)
      .then(cache => cache.addAll(ASSETS))
  );
});

self.addEventListener('activate', event => {
  event.waitUntil(
    caches.keys().then(keys =>
      Promise.all(
        keys.filter(key => key !== CACHE_NAME)
          .map(key => caches.delete(key))
      )
    )
  );
});

self.addEventListener('fetch', event => {
  event.respondWith(
    caches.match(event.request)
      .then(cached => cached || fetch(event.request)
        .then(response => {
          const clone = response.clone();
          caches.open(CACHE_NAME)
            .then(cache => cache.put(event.request, clone));
          return response;
        })
      )
  );
});</code></pre>
                                <h2>Caching Strategies</h2>
                                <p><strong>Cache First:</strong> Check cache before network. Best for static assets.
                                    <strong>Network First:</strong> Try network, fall back to cache. Best for dynamic
                                    content. <strong>Stale While Revalidate:</strong> Serve from cache immediately,
                                    update cache in background. Best for content that can be slightly stale.
                                    <strong>Cache Only:</strong> Only serve from cache. For truly static assets.
                                    <strong>Network Only:</strong> Bypass cache entirely. For real-time data like
                                    authentication.
                                </p>
                                <h2>Performance Optimization</h2>
                                <p>Measure with Lighthouse. Implement code splitting and lazy loading to reduce initial
                                    bundle size. Optimize images with modern formats (WebP, AVIF) and responsive sizing.
                                    Use CDN for static assets. Follow the PRPL pattern (Push, Render, Pre-cache,
                                    Lazy-load) for optimal loading strategy. Monitor Core Web Vitals (LCP under 2.5s,
                                    FID under 100ms, CLS under 0.1) as they directly impact user experience and search
                                    ranking.</p>
                            </div>
                        </div>
                        <button class="blog-read-more"><span>Read More</span> <i
                                class="fas fa-chevron-down"></i></button>
                    </div>

                    <!-- Web Article 3: API Security -->
                    <div class="blog-card reveal">
                        <div class="blog-card-header">
                            <h3>REST API Security: Authentication, Rate Limiting and Input Validation</h3>
                            <span class="tag"
                                style="background:rgba(34,197,94,0.1);color:#4ade80;border-color:rgba(34,197,94,0.2);">Web</span>
                        </div>
                        <p>Comprehensive guide to building secure REST APIs with proper authentication, authorization,
                            and protection against common attacks.</p>
                        <div class="blog-excerpt">
                            <div class="blog-content article-content">
                                <h2>Authentication Patterns</h2>
                                <p><strong>API Keys:</strong> Simple but limited. Best for identifying applications
                                    rather than users. Should be transmitted in headers, never in URLs. Rotate regularly
                                    and scope to minimum required permissions. <strong>OAuth 2.0:</strong> Industry
                                    standard for delegated authorization. Use PKCE flow for public clients (SPAs, mobile
                                    apps). Authorization code flow with client secret for server-side apps. Never use
                                    implicit flow as it exposes tokens in URLs.</p>
                                <p><strong>JWT Best Practices:</strong> Use short expiration times (15-30 minutes).
                                    Store in HttpOnly cookies, not localStorage (prevents XSS theft). Include only
                                    necessary claims in the payload. Always validate the signature, issuer, audience,
                                    and expiration on the server side. Use RS256 (asymmetric) over HS256 (symmetric)
                                    when multiple services need to verify tokens.</p>
                                <h2>Rate Limiting</h2>
                                <p>Implement at multiple levels: reverse proxy (nginx), application layer, and
                                    per-user/per-IP. Token bucket and sliding window algorithms are most common. Return
                                    429 status with Retry-After header. Rate limiting prevents abuse, protects against
                                    DDoS, and ensures fair resource allocation. Consider different limits for
                                    authenticated vs unauthenticated requests.</p>
                                <h2>Input Validation</h2>
                                <p>Validate all inputs on the server side, never trust client-side validation. Use
                                    schema validation libraries (Joi, Zod, Pydantic) to enforce types, ranges, formats,
                                    and lengths. Reject unexpected fields (allowlist approach). Sanitize inputs that
                                    will be rendered in HTML or used in shell commands. For file uploads, validate MIME
                                    type (do not trust Content-Type header), check magic bytes, enforce size limits, and
                                    scan for malware.</p>
                                <h2>Error Handling</h2>
                                <p>Never expose internal errors, stack traces, or database queries in API responses. Use
                                    generic error messages for clients and detailed logging for developers. Return
                                    appropriate HTTP status codes. Implement a global error handler that catches
                                    unhandled exceptions and converts them to safe 500 responses.</p>
                            </div>
                        </div>
                        <button class="blog-read-more"><span>Read More</span> <i
                                class="fas fa-chevron-down"></i></button>
                    </div>

                    <!-- Article: Docker Containerization -->
                    <div class="blog-card reveal">
                        <div class="blog-card-header">
                            <h3>Docker Containerization: From Development to Production</h3>
                            <span class="tag"
                                style="background:rgba(59,130,246,0.1);color:#60a5fa;border-color:rgba(59,130,246,0.2);">DevOps</span>
                        </div>
                        <p>A comprehensive guide to building, optimizing, and deploying Docker containers for real-world
                            applications.</p>
                        <div class="blog-excerpt">
                            <div class="blog-content article-content">
                                <h2>Why Containers Matter</h2>
                                <p>Containers have fundamentally changed how we develop, ship, and run software. Instead
                                    of the traditional "it works on my machine" problem, containers package your
                                    application with all its dependencies into a single, portable unit. The key insight
                                    is that containers share the host kernel while maintaining process isolation through
                                    Linux namespaces and cgroups, making them dramatically lighter than virtual
                                    machines.</p>
                                <h2>Writing Efficient Dockerfiles</h2>
                                <p>Every instruction creates a layer, and layer management directly impacts build time
                                    and image size. Multi-stage builds are essential: the build stage contains all
                                    development dependencies, while the production stage only contains the compiled
                                    output. This can reduce image size by 90% or more. Order instructions from
                                    least-changing to most-changing. Package files rarely change, so copying and
                                    installing them first means Docker can cache those layers.</p>
                                <pre><code>FROM node:20-alpine AS builder
WORKDIR /app
COPY package*.json ./
RUN npm ci --only=production
COPY . .
RUN npm run build

FROM node:20-alpine AS production
WORKDIR /app
COPY --from=builder /app/dist ./dist
COPY --from=builder /app/node_modules ./node_modules
USER node
CMD ["node", "dist/server.js"]</code></pre>
                                <h2>Container Security</h2>
                                <p>Never run containers as root. Use the USER directive to drop privileges. Scan images
                                    for vulnerabilities with tools like Trivy or Grype. Use distroless base images for
                                    production to minimize attack surface. Pin your base image versions to avoid
                                    unexpected changes from upstream.</p>
                                <h2>Docker Compose and Orchestration</h2>
                                <p>Docker Compose defines multi-container applications in a single YAML file. Define
                                    services, networks, and volumes declaratively. Use profiles to conditionally start
                                    services (e.g., debugging tools only in development). For production, Kubernetes or
                                    Docker Swarm handle orchestration: auto-scaling, rolling updates, health checks, and
                                    service discovery. Start with Compose for development, graduate to Kubernetes when
                                    your deployment complexity demands it.</p>
                                <h2>Networking and Service Discovery</h2>
                                <p>Docker creates isolated bridge networks by default. Containers on the same network
                                    can resolve each other by service name. Use custom networks to isolate groups of
                                    containers: your frontend talks to API, API talks to database, but frontend never
                                    directly accesses the database. This network segmentation mirrors production
                                    architecture and catches connectivity issues early.</p>
                                <p>Port mapping (<code>-p 8080:3000</code>) exposes container ports to the host. In
                                    Compose, only expose ports that need external access. Internal service-to-service
                                    communication stays within the Docker network without port mapping, which is both
                                    more secure and more performant.</p>
                                <h2>Volumes and Data Persistence</h2>
                                <p>Containers are ephemeral. Named volumes persist data across container restarts and
                                    recreations. Bind mounts map host directories into containers, essential for
                                    development hot-reload workflows. For databases, always use named volumes. For
                                    development, bind mount your source code directory. For production, consider using
                                    volume drivers that integrate with cloud storage (EBS, Azure Disk, NFS).</p>
                                <h2>Health Checks and Debugging</h2>
                                <p>Add HEALTHCHECK instructions to your Dockerfile. Docker will periodically run the
                                    health check command and mark the container as healthy, unhealthy, or starting.
                                    Orchestrators use this to make routing and restart decisions. For debugging:
                                    <code>docker exec -it container_name sh</code> gives you a shell inside a running
                                    container. <code>docker logs -f container_name</code> streams logs.
                                    <code>docker stats</code> shows real-time resource usage per container.
                                </p>
                                <h2>Production Best Practices</h2>
                                <p>Use .dockerignore to exclude unnecessary files (node_modules, .git, test files) from
                                    the build context. Set memory and CPU limits with <code>--memory</code> and
                                    <code>--cpus</code> flags. Implement graceful shutdown handling: your application
                                    should listen for SIGTERM and clean up connections before exiting. Use Docker
                                    BuildKit for parallel builds and better caching. Tag images with git commit SHA for
                                    traceability, not just "latest".
                                </p>
                            </div>
                        </div>
                        <button class="blog-read-more"><span>Read More</span> <i
                                class="fas fa-chevron-down"></i></button>
                    </div>

                    <!-- Article: Git Workflow -->
                    <div class="blog-card reveal">
                        <div class="blog-card-header">
                            <h3>Advanced Git Workflows: Branching Strategies and Collaboration</h3>
                            <span class="tag"
                                style="background:rgba(59,130,246,0.1);color:#60a5fa;border-color:rgba(59,130,246,0.2);">Dev</span>
                        </div>
                        <p>Mastering Git branching strategies, rebasing, and collaborative workflows for professional
                            development.</p>
                        <div class="blog-excerpt">
                            <div class="blog-content article-content">
                                <h2>Branching Strategies</h2>
                                <p><strong>Git Flow:</strong> Uses long-lived branches (main, develop, feature, release,
                                    hotfix). Best for projects with scheduled releases but can become overly complex for
                                    small teams.</p>
                                <p><strong>GitHub Flow:</strong> Simple model with main branch and feature branches.
                                    Best for continuous deployment. Every merge to main should be deployable.</p>
                                <p><strong>Trunk-Based Development:</strong> Developers commit directly to trunk or use
                                    very short-lived feature branches. Requires feature flags, strong CI/CD, and high
                                    test coverage. Used by Google and Facebook.</p>
                                <h2>Rebasing vs Merging</h2>
                                <p>Interactive rebase (<code>git rebase -i</code>) is my most-used Git feature, it
                                    allows squashing, reordering, and editing commits before sharing them. The golden
                                    rule: never rebase shared branches. Rebase your local feature branch onto main
                                    before creating a PR, but never rebase main onto anything.</p>
                                <h2>Commit Conventions and Changelog Automation</h2>
                                <p>Conventional Commits format (<code>type(scope): description</code>) enables automated
                                    changelog generation and semantic versioning. Types include: feat, fix, docs, style,
                                    refactor, test, and chore. The key insight is that commit messages should explain
                                    WHY a change was made, not WHAT was changed. The diff already shows what changed.
                                    Each commit should represent a single logical change that could be independently
                                    reverted if needed.</p>
                                <pre><code>feat(auth): add OAuth2 PKCE flow for mobile clients
fix(api): handle race condition in concurrent user updates
refactor(db): migrate from raw SQL to query builder
chore(deps): bump express from 4.18.2 to 4.19.0</code></pre>
                                <p>Tools like <code>commitlint</code> enforce conventions in CI, and
                                    <code>standard-version</code> or <code>semantic-release</code> automate version
                                    bumping and changelog generation based on commit types. This turns your Git history
                                    into a reliable, machine-readable record of changes.
                                </p>
                                <h2>Code Review Best Practices</h2>
                                <p>Pull requests should be small (under 400 lines). Large PRs get rubber-stamped, small
                                    PRs get thorough reviews. Use draft PRs for early feedback on approach before
                                    investing time in polish. Automated checks (lint, tests, type checking, security
                                    scans) should run before human review so reviewers can focus on architecture, edge
                                    cases, and maintainability rather than style issues.</p>
                                <p>A good code review is a conversation, not an approval stamp. Reviewers should explain
                                    the reasoning behind suggestions. Authors should provide context in the PR
                                    description: what problem this solves, why this approach was chosen over
                                    alternatives, and what edge cases were considered. Link to relevant issues or design
                                    documents. Screenshots or recordings for UI changes are invaluable.</p>
                                <h2>CI/CD Integration</h2>
                                <p>Git workflows are only as good as the CI/CD pipeline backing them. Every push should
                                    trigger: linting, type checking, unit tests, integration tests, and security scans.
                                    For main branch pushes, add deployment stages. Use branch protection rules to
                                    prevent direct pushes to main and require at least one approved review. Status
                                    checks must pass before merging. This automated quality gate is what makes
                                    trunk-based development viable at scale.</p>
                                <h2>Monorepo Strategies</h2>
                                <p>Monorepos (single repository for multiple projects) are gaining popularity. Tools
                                    like Nx, Turborepo, and Bazel handle the complexity of building and testing only
                                    affected projects. The advantage is atomic changes across multiple packages, shared
                                    tooling, and simplified dependency management. The disadvantage is that CI gets
                                    complex, and you need sophisticated caching to keep build times reasonable. Use
                                    CODEOWNERS files to enforce team-specific review requirements for different parts of
                                    the repo.</p>
                            </div>
                        </div>
                        <button class="blog-read-more"><span>Read More</span> <i
                                class="fas fa-chevron-down"></i></button>
                    </div>

                    <!-- Article: Python Async -->
                    <div class="blog-card reveal">
                        <div class="blog-card-header">
                            <h3>Python Async Programming: Concurrency with asyncio</h3>
                            <span class="tag"
                                style="background:rgba(168,85,247,0.1);color:#c084fc;border-color:rgba(168,85,247,0.2);">Python</span>
                        </div>
                        <p>Understanding Python's asyncio, event loops, and coroutines for high-performance I/O-bound
                            applications.</p>
                        <div class="blog-excerpt">
                            <div class="blog-content article-content">
                                <h2>The Problem asyncio Solves</h2>
                                <p>Python's GIL prevents true parallelism for CPU-bound tasks. However, most real-world
                                    applications are I/O-bound. asyncio allows a single thread to handle thousands of
                                    concurrent I/O operations by switching between tasks during wait periods. A
                                    synchronous client handling 1000 sequential API calls might take 100 seconds. An
                                    async version can complete the same work in 2-3 seconds.</p>
                                <h2>Coroutines and the Event Loop</h2>
                                <pre><code>import asyncio
import aiohttp

async def fetch_url(session, url):
    async with session.get(url) as response:
        return await response.json()

async def main():
    urls = [f"https://api.example.com/item/{i}" for i in range(100)]
    async with aiohttp.ClientSession() as session:
        tasks = [fetch_url(session, url) for url in urls]
        results = await asyncio.gather(*tasks)
    return results

asyncio.run(main())</code></pre>
                                <h2>Common Patterns</h2>
                                <p><strong>Producer-Consumer:</strong> Use <code>asyncio.Queue</code> for safe
                                    communication between coroutines. Multiple producers add items, multiple consumers
                                    process them. This pattern is ideal for web scraping, data pipelines, and task
                                    processing systems where you want to decouple data acquisition from data processing.
                                </p>
                                <p><strong>Timeouts:</strong> Always use <code>asyncio.wait_for()</code> or
                                    <code>asyncio.timeout()</code> (Python 3.11+) to prevent coroutines from hanging
                                    indefinitely. Network operations can stall forever without timeouts. Set realistic
                                    timeout values based on your expected response times and add retry logic with
                                    exponential backoff for resilience.
                                </p>
                                <p><strong>Avoid blocking calls:</strong> A single blocking call like
                                    <code>time.sleep()</code> or a synchronous database query blocks the entire event
                                    loop, freezing all concurrent operations. Use <code>asyncio.to_thread()</code>
                                    (Python 3.9+) to run blocking code in a thread pool. This is the single most common
                                    mistake in async Python code.
                                </p>
                                <p><strong>Rate Limiting:</strong> Use <code>asyncio.Semaphore</code> to limit
                                    concurrent operations. If you are hitting an API with a rate limit of 100 requests
                                    per second, create a semaphore with that limit and acquire it before each request.
                                    This prevents overwhelming external services while maximizing throughput within
                                    acceptable limits.</p>
                                <h2>Debugging Async Code</h2>
                                <p>Async bugs are notoriously difficult to diagnose. Enable debug mode with
                                    <code>asyncio.run(main(), debug=True)</code> to get warnings about slow coroutines
                                    and unawaited tasks. Use <code>asyncio.all_tasks()</code> to inspect running tasks.
                                    The <code>aiomonitor</code> library provides a telnet-based monitor for inspecting
                                    the event loop in production. For testing, <code>pytest-asyncio</code> handles async
                                    test fixtures and test functions.
                                </p>
                                <h2>When Not to Use asyncio</h2>
                                <p>asyncio is not a silver bullet. For CPU-bound work (image processing, ML inference,
                                    heavy computation), use <code>multiprocessing</code> or
                                    <code>concurrent.futures.ProcessPoolExecutor</code>. For simple scripts with minimal
                                    I/O, synchronous code is clearer and easier to debug. Use async when you have many
                                    concurrent I/O operations: API calls, database queries, file operations, websocket
                                    connections.
                                </p>
                                <p>Also consider: if your application only makes a handful of concurrent requests, the
                                    complexity of async is not worth it. <code>requests</code> with
                                    <code>ThreadPoolExecutor</code> is simpler and works fine for moderate concurrency.
                                    Reserve asyncio for cases where you truly need thousands of concurrent connections
                                    or sub-millisecond scheduling.
                                </p>
                            </div>
                        </div>
                        <button class="blog-read-more"><span>Read More</span> <i
                                class="fas fa-chevron-down"></i></button>
                    </div>

                    <!-- Article: Linux CLI Mastery -->
                    <div class="blog-card reveal">
                        <div class="blog-card-header">
                            <h3>Linux Command Line Mastery: Essential Tools and Techniques</h3>
                            <span class="tag"
                                style="background:rgba(34,197,94,0.1);color:#4ade80;border-color:rgba(34,197,94,0.2);">Linux</span>
                        </div>
                        <p>Advanced command line techniques, text processing, and scripting patterns for Linux
                            developers.</p>
                        <div class="blog-excerpt">
                            <div class="blog-content article-content">
                                <h2>Text Processing Pipeline</h2>
                                <p>The Unix philosophy of "small tools that do one thing well" creates an incredibly
                                    powerful text processing pipeline. Combining grep, sed, awk, sort, uniq, cut, and tr
                                    with pipes can replace hundreds of lines of Python or JavaScript for data
                                    manipulation.</p>
                                <pre><code># Find top 10 IPs making requests
cat access.log | awk '{print $1}' | sort | uniq -c | sort -rn | head -10

# Replace all occurrences in multiple files
find . -name "*.py" -exec sed -i 's/old_function/new_function/g' {} +

# Process CSV: extract column, filter, count unique values
cut -d',' -f3 data.csv | grep -v "^$" | sort | uniq -c | sort -rn</code></pre>
                                <h2>Process Management and Debugging</h2>
                                <p><code>strace</code> traces system calls and is invaluable for debugging. If an
                                    application fails silently, <code>strace -f -e trace=file,network ./app</code> shows
                                    you exactly which files it tries to open and which network connections it attempts.
                                    <code>ltrace</code> does the same for library calls.
                                </p>
                                <p><code>lsof</code> shows open files and network connections per process.
                                    <code>lsof -i :8080</code> tells you what is using port 8080.
                                    <code>lsof -p PID</code> lists everything a process has open. Combined with
                                    <code>ss -tlnp</code> for socket statistics, you have complete network visibility.
                                </p>
                                <p><code>tmux</code> provides persistent terminal sessions that survive SSH
                                    disconnections. Key bindings: <code>Ctrl-b c</code> (new window),
                                    <code>Ctrl-b d</code> (detach), <code>tmux attach</code> (reconnect). Session
                                    management is essential for long-running tasks on remote servers.
                                </p>
                                <p><code>htop</code> and <code>btop</code> provide real-time process monitoring. For
                                    container environments, <code>ctop</code> shows per-container resource usage. Use
                                    <code>journalctl -u service-name -f</code> for real-time log monitoring of systemd
                                    services.
                                </p>
                                <h2>Shell Scripting Best Practices</h2>
                                <p>Start every Bash script with <code>set -euo pipefail</code>. This combination is
                                    critical: <code>-e</code> exits on any error, <code>-u</code> treats unset variables
                                    as errors, and <code>-o pipefail</code> catches errors in piped commands (normally
                                    only the last command's exit code is checked). Always use <code>shellcheck</code>
                                    for static analysis, it catches dozens of common pitfalls.</p>
                                <p>Quote all variables to prevent word splitting: <code>"$variable"</code> instead of
                                    <code>$variable</code>. Use functions with <code>local</code> variables for
                                    modularity. Trap signals for cleanup: <code>trap cleanup EXIT</code> ensures
                                    temporary files are removed even if the script fails or is interrupted.
                                </p>
                                <pre><code>#!/bin/bash
set -euo pipefail

tmp_dir=$(mktemp -d)
trap 'rm -rf "$tmp_dir"' EXIT

process_file() {
    local input="$1"
    local output="${tmp_dir}/$(basename "$input").processed"
    grep -v '^#' "$input" | sort -u > "$output"
    echo "$output"
}

for f in "$@"; do
    result=$(process_file "$f")
    echo "Processed: $result"
done</code></pre>
                                <h2>Performance Analysis</h2>
                                <p>Use <code>time</code> for basic timing, <code>perf stat</code> for CPU counter
                                    analysis, and <code>perf record</code> + <code>perf report</code> for profiling.
                                    <code>valgrind --tool=callgrind</code> generates call graphs for C/C++ programs. For
                                    I/O intensive applications, <code>iostat</code> and <code>iotop</code> show disk
                                    utilization patterns. Understanding where bottlenecks are is the first step to
                                    optimization.
                                </p>
                            </div>
                        </div>
                        <button class="blog-read-more"><span>Read More</span> <i
                                class="fas fa-chevron-down"></i></button>
                    </div>

                    <!-- New Article: Neural Networks from Scratch -->
                    <div class="blog-card reveal">
                        <div class="blog-card-header">
                            <h3>Building a Neural Network from Scratch with NumPy</h3>
                            <span class="tag"
                                style="background:rgba(168,85,247,0.1);color:#c084fc;border-color:rgba(168,85,247,0.2);">AI/ML</span>
                        </div>
                        <p>Implementing forward propagation, backpropagation, and gradient descent with only NumPy.</p>
                        <div class="blog-excerpt">
                            <div class="blog-content article-content">
                                <h2>Why Build from Scratch</h2>
                                <p>Frameworks like PyTorch abstract away the mathematics, which is great for
                                    productivity but terrible for understanding. When you implement backpropagation by
                                    hand, you truly understand why certain architectures work, why gradients vanish or
                                    explode, and why specific initialization strategies matter.</p>
                                <h2>Forward Propagation</h2>
                                <p>A neural network is a series of matrix multiplications followed by non-linear
                                    activation functions. For a single layer: Z = W @ X + b, then A = activation(Z).</p>
                                <pre><code>import numpy as np

class NeuralNetwork:
    def __init__(self, layers):
        self.weights = []
        self.biases = []
        for i in range(len(layers) - 1):
            w = np.random.randn(layers[i+1], layers[i]) * np.sqrt(2.0 / layers[i])
            b = np.zeros((layers[i+1], 1))
            self.weights.append(w)
            self.biases.append(b)

    def forward(self, X):
        self.activations = [X]
        A = X
        for i, (W, b) in enumerate(zip(self.weights, self.biases)):
            Z = W @ A + b
            A = np.maximum(0, Z) if i < len(self.weights) - 1 else Z
            self.activations.append(A)
        return A</code></pre>
                                <h2>Backpropagation</h2>
                                <p>Backpropagation is just the chain rule of calculus applied systematically. Starting
                                    from the loss, we compute the gradient of each weight. For each layer: dW = dZ @
                                    A_prev.T, db = sum(dZ), and dA_prev = W.T @ dZ. This recursive structure makes the
                                    algorithm elegant and efficient.</p>
                                <h2>Training and Optimization</h2>
                                <p>Vanilla gradient descent updates W = W - lr * dW. In practice, optimizers like Adam
                                    maintain per-parameter learning rates and momentum. Adam combines the ideas of
                                    RMSProp (adaptive learning rates) and momentum (exponential moving average of
                                    gradients). It works well with default hyperparameters (lr=0.001, beta1=0.9,
                                    beta2=0.999) which is why it became the default choice.</p>
                                <p>Weight initialization matters enormously. He initialization (scale by sqrt(2/n_in))
                                    is correct for ReLU activations, while Xavier initialization (scale by sqrt(1/n_in))
                                    is for tanh and sigmoid. Bad initialization can make a network completely
                                    untrainable, with gradients vanishing (weights too small) or exploding (weights too
                                    large) in the first few forward passes.</p>
                                <h2>Regularization Techniques</h2>
                                <p>Overfitting is the enemy of generalization. L2 regularization adds a penalty term
                                    proportional to the squared magnitude of weights (weight decay), encouraging smaller
                                    weights. Dropout randomly zeroes out neurons during training, forcing the network to
                                    learn redundant representations. Batch Normalization normalizes layer inputs,
                                    stabilizing training and allowing higher learning rates. Data augmentation
                                    artificially expands the training set through transformations like rotation,
                                    flipping, and color jittering.</p>
                                <h2>Loss Functions and Evaluation</h2>
                                <p>For classification, Cross-Entropy Loss measures the divergence between predicted
                                    probability distribution and true labels. For regression, Mean Squared Error (MSE)
                                    or Mean Absolute Error (MAE) are standard choices. The choice of loss function
                                    directly affects what the model optimizes for. MSE penalizes large errors
                                    quadratically, making the model sensitive to outliers. MAE treats all errors
                                    equally. Huber loss combines both, being quadratic for small errors and linear for
                                    large ones.</p>
                                <p>Monitor both training and validation loss during training. If training loss decreases
                                    but validation loss increases, you are overfitting. If both are high, the model is
                                    underfitting, either increase capacity or train longer. The gap between training and
                                    validation performance is the key diagnostic.</p>
                            </div>
                        </div>
                        <button class="blog-read-more"><span>Read More</span> <i
                                class="fas fa-chevron-down"></i></button>
                    </div>

                    <!-- Micro-Frontends Article -->
                    <div class="blog-card reveal">
                        <div class="blog-card-header">
                            <h3>Modern Web Architecture: Monolithic to Micro-Frontends</h3>
                            <span class="tag"
                                style="background:rgba(59,130,246,0.1);color:#60a5fa;border-color:rgba(59,130,246,0.2);">Web</span>
                        </div>
                        <p>How and why modern web development is migrating towards micro-frontend architectures.</p>
                        <div class="blog-excerpt">
                            <div class="blog-content article-content">
                                <h2>The Problem with Monoliths</h2>
                                <p>We moved backends from monoliths to microservices, but the frontend remained a large
                                    SPA. As applications grow, these frontend monoliths become difficult to maintain and
                                    slow to build. A small update requires testing and deploying the entire application.
                                </p>
                                <h2>Module Federation</h2>
                                <p>Webpack 5 introduced Module Federation, the de-facto standard for micro-frontends. It
                                    allows dynamically loading code from another application at runtime while
                                    intelligently sharing dependencies. If both host and remote use React, the shared
                                    dependency is downloaded only once.</p>
                                <h2>Challenges and State Management</h2>
                                <p>State sharing between micro-frontends should be minimized. Each micro-frontend owns
                                    its state and communicates through well-defined interfaces. Use custom events for
                                    loosely coupled communication, URL parameters for routing-based state, and a thin
                                    shared state layer (via context or a lightweight store) only for truly global
                                    concerns like user authentication.</p>
                                <p>A shared component library or Design Tokens must be enforced across teams for
                                    consistent styling. Without this, the user experience will feel fragmented. Publish
                                    your design system as an npm package that all micro-frontends consume. Use semantic
                                    versioning so teams can upgrade at their own pace.</p>
                                <h2>Build and Deploy Pipeline</h2>
                                <p>Each micro-frontend should have its own CI/CD pipeline, repository, and deploy cycle.
                                    This is the entire point: independent deployability. Use feature flags to control
                                    rollout. Blue-green or canary deployments let you roll back individual
                                    micro-frontends without affecting others. Shared dependencies (React, design system)
                                    should be loaded once via Module Federation or import maps.</p>
                                <h2>Performance Considerations</h2>
                                <p>Multiple independently deployed applications can bloat page size if dependencies are
                                    not shared properly. Use Module Federation's shared configuration to deduplicate
                                    common libraries. Lazy load micro-frontends below the fold. Monitor Core Web Vitals
                                    per-page to catch regressions. Server-side composition (e.g., with Podium or Tailor)
                                    can improve initial load performance compared to client-side composition.</p>
                                <h2>When to Use Micro-Frontends</h2>
                                <p>Micro-frontends are not for small teams or simple applications. The overhead of
                                    maintaining multiple repositories, build pipelines, and deployment processes is
                                    significant. They make sense when: multiple teams need to work on the same
                                    application independently, different parts of the application have different release
                                    cadences, or you need technology diversity (one team uses React, another uses Vue).
                                    For most projects, a well-organized monolith is simpler and faster to develop.</p>
                            </div>
                        </div>
                        <button class="blog-read-more"><span>Read More</span> <i
                                class="fas fa-chevron-down"></i></button>
                    </div>

                    <!-- Mobile App Security Article -->
                    <div class="blog-card reveal">
                        <div class="blog-card-header">
                            <h3>Mobile App Security: Reverse Engineering and Defenses</h3>
                            <span class="tag"
                                style="background:rgba(59,130,246,0.1);color:#60a5fa;border-color:rgba(59,130,246,0.2);">Mobile</span>
                        </div>
                        <p>How attackers dissect mobile applications and the techniques developers use to stop them.</p>
                        <div class="blog-excerpt">
                            <div class="blog-content article-content">
                                <h2>The Threat Landscape</h2>
                                <p>Mobile applications reside on the user's device, meaning the attacker has physical
                                    access to the binary. An attacker with a rooted or jailbroken device has complete
                                    control over memory, file system, and network traffic.</p>
                                <h2>Reverse Engineering</h2>
                                <p>For Android, APK is a ZIP containing DEX, resources, and manifest. Tools like apktool
                                    and JADX decompile back to readable Java code. For iOS, Ghidra or IDA Pro handle ARM
                                    assembly analysis. Frida allows hooking into running functions in real-time.</p>
                                <h2>Defense: Obfuscation and RASP</h2>
                                <p>Code obfuscation is the first line of defense. ProGuard (free) renames classes and
                                    removes unused code. R8 (Google's successor) is more aggressive. DexGuard
                                    (commercial) adds string encryption, reflection-based API hiding, and control flow
                                    obfuscation. The goal is not to make reverse engineering impossible (it never is),
                                    but to make it expensive enough that most attackers give up.</p>
                                <p>RASP (Runtime Application Self-Protection) adds active defense. At launch, the app
                                    checks for: root/jailbreak status, emulator detection (checking for Goldfish kernel,
                                    QEMU properties), hooking frameworks like Frida or Xposed, debugger attachment
                                    (ptrace checks), and APK signature verification. If any check fails, the app can
                                    refuse to run, wipe sensitive data, or silently alter its behavior to serve fake
                                    data to the attacker.</p>
                                <h2>Certificate Pinning</h2>
                                <p>HTTPS alone is not enough. On a rooted device, an attacker can install a custom CA
                                    certificate and intercept all HTTPS traffic with tools like mitmproxy or Charles
                                    Proxy. Certificate Pinning hardcodes the expected certificate hash (pin) directly in
                                    the app. During TLS handshake, if the presented certificate does not match the
                                    pinned hash, the connection is refused immediately. This renders MitM proxies
                                    useless even on rooted devices.</p>
                                <p>The downside: if your certificate rotates and you forget to ship an update with the
                                    new pin, your app breaks for all users. Use backup pins and set reasonable pin
                                    expiration windows. OkHttp and TrustKit make implementing pinning straightforward on
                                    Android and iOS respectively.</p>
                                <h2>Secure Data Storage</h2>
                                <p>Never store sensitive data in SharedPreferences or UserDefaults in plaintext. Use
                                    Android Keystore or iOS Keychain for cryptographic keys. Encrypt local databases
                                    with SQLCipher. Avoid logging sensitive information in production builds. Clear the
                                    clipboard after pasting sensitive data. Disable screenshots for screens showing
                                    sensitive information using FLAG_SECURE on Android.</p>
                                <h2>Network Security Best Practices</h2>
                                <p>Use Network Security Config on Android to restrict cleartext traffic and customize
                                    trust anchors. Implement token-based authentication (JWT or OAuth2) with short-lived
                                    access tokens and refresh tokens. Validate all server responses, never trust the
                                    server blindly. Implement request signing for critical API calls to prevent replay
                                    attacks. Rate limit sensitive operations on both client and server side.</p>
                            </div>
                        </div>
                        <button class="blog-read-more"><span>Read More</span> <i
                                class="fas fa-chevron-down"></i></button>
                    </div>

                    <!-- ========================= -->

                    <!-- AI Article 1: RAG Pipelines -->
                    <div class="blog-card reveal">
                        <div class="blog-card-header">
                            <h3>Building Production RAG Pipelines: From Documents to Answers</h3>
                            <span class="tag"
                                style="background:rgba(168,85,247,0.1);color:#c084fc;border-color:rgba(168,85,247,0.2);">AI/ML</span>
                        </div>
                        <p>A deep technical guide to building Retrieval-Augmented Generation systems that actually work
                            in production.</p>
                        <div class="blog-excerpt">
                            <div class="blog-content">
                                <p>Retrieval-Augmented Generation (RAG) has become the go-to architecture for building
                                    AI systems that can answer questions about your own data. Instead of fine-tuning an
                                    LLM on your documents (expensive, slow, quickly outdated), RAG retrieves relevant
                                    chunks at query time and feeds them to the LLM as context. Simple concept, but the
                                    implementation details make all the difference.</p>

                                <h2>Document Processing Pipeline</h2>
                                <p>The first stage is document ingestion. Raw documents (PDFs, web pages, Markdown, code
                                    files) need to be parsed into clean text. For PDFs, libraries like
                                    <code>PyMuPDF</code> (fitz) or <code>unstructured</code> handle extraction, but OCR
                                    quality varies wildly. Always inspect your parsed output, garbage text leads to
                                    garbage embeddings leads to garbage retrieval.
                                </p>
                                <pre><code>import fitz  # PyMuPDF

def extract_text_from_pdf(path):
    doc = fitz.open(path)
    pages = []
    for page in doc:
        text = page.get_text("text")
        # Clean whitespace and normalize
        text = " ".join(text.split())
        if len(text) > 50:  # Skip near-empty pages
            pages.append({"page": page.number, "text": text})
    return pages</code></pre>

                                <h2>Chunking Strategy</h2>
                                <p>Chunking is where most RAG pipelines fail. Naive fixed-size chunking (e.g., 500
                                    tokens) splits sentences mid-thought and destroys context. Better approaches include
                                    recursive character splitting with overlap, semantic chunking based on embedding
                                    similarity between sentences, or structure-aware chunking that respects headers,
                                    paragraphs, and code blocks.</p>
                                <pre><code>from langchain.text_splitter import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=800,
    chunk_overlap=200,
    separators=["\n\n", "\n", ". ", " ", ""],
    length_function=len,
)
chunks = splitter.split_documents(docs)</code></pre>
                                <p>The chunk_overlap parameter is critical. Too little overlap and you lose context at
                                    boundaries. Too much and you waste embedding space with redundant content. 20-25%
                                    overlap is a good starting point. For code, use AST-aware chunking that splits at
                                    function/class boundaries.</p>

                                <h2>Embedding and Vector Store</h2>
                                <p>Each chunk gets converted into a dense vector embedding. The embedding model matters
                                    more than most people think. OpenAI's <code>text-embedding-3-small</code> is decent,
                                    but for technical content, models like <code>BAAI/bge-large-en-v1.5</code> or
                                    <code>nomic-embed-text-v1.5</code> often produce better results. Always benchmark
                                    with your actual data.
                                </p>
                                <pre><code>from sentence_transformers import SentenceTransformer
import chromadb

model = SentenceTransformer("BAAI/bge-large-en-v1.5")
client = chromadb.PersistentClient(path="./chroma_db")
collection = client.get_or_create_collection(
    name="docs",
    metadata={"hnsw:space": "cosine"}
)

embeddings = model.encode(texts, normalize_embeddings=True)
collection.add(
    documents=texts,
    embeddings=embeddings.tolist(),
    ids=[f"chunk_{i}" for i in range(len(texts))],
    metadatas=[{"source": s} for s in sources]
)</code></pre>

                                <h2>Retrieval and Reranking</h2>
                                <p>Semantic search alone is not enough. The initial retrieval (top-k nearest neighbors)
                                    often includes irrelevant results. A reranking step using a cross-encoder model like
                                    <code>cross-encoder/ms-marco-MiniLM-L-12-v2</code> dramatically improves precision.
                                    Retrieve top-20, rerank, then use top-5 for context.
                                </p>
                                <p>Hybrid search combining dense (semantic) and sparse (BM25/TF-IDF) retrieval
                                    consistently outperforms either alone. The sparse component catches exact keyword
                                    matches that embedding models sometimes miss, while the dense component handles
                                    semantic similarity.</p>
                                <pre><code>from sentence_transformers import CrossEncoder

reranker = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-12-v2")

def retrieve_and_rerank(query, top_k=20, final_k=5):
    results = collection.query(query_texts=[query], n_results=top_k)
    docs = results["documents"][0]

    pairs = [[query, doc] for doc in docs]
    scores = reranker.predict(pairs)

    ranked = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)
    return [doc for doc, score in ranked[:final_k]]</code></pre>

                                <h2>Production Considerations</h2>
                                <p>In production, you need to handle: document updates (re-embed changed chunks, delete
                                    stale ones), metadata filtering (filter by source, date, category before semantic
                                    search), query transformation (expand ambiguous queries, handle multi-step
                                    questions), and answer grounding (cite which chunks were used, detect
                                    hallucination). Evaluation is also critical: use frameworks like RAGAS to measure
                                    context relevance, answer faithfulness, and answer relevancy across your test
                                    queries.</p>
                            </div>
                        </div>
                        <button class="blog-read-more"><span>Read More</span> <i
                                class="fas fa-chevron-down"></i></button>
                    </div>

                    <!-- AI Article 2: LoRA Fine-Tuning -->
                    <div class="blog-card reveal">
                        <div class="blog-card-header">
                            <h3>LoRA and QLoRA: Efficient LLM Fine-Tuning on Consumer Hardware</h3>
                            <span class="tag"
                                style="background:rgba(168,85,247,0.1);color:#c084fc;border-color:rgba(168,85,247,0.2);">AI/ML</span>
                        </div>
                        <p>How to fine-tune 7B-70B parameter models on a single GPU using LoRA, QLoRA, and modern
                            training techniques.</p>
                        <div class="blog-excerpt">
                            <div class="blog-content">
                                <p>Fine-tuning large language models used to require multiple A100 GPUs and thousands of
                                    dollars in compute. LoRA (Low-Rank Adaptation) changed everything by freezing the
                                    base model weights and injecting small trainable matrices into the attention layers.
                                    Instead of training 7 billion parameters, you train maybe 10-50 million, a 99%
                                    reduction in trainable parameters.</p>

                                <h2>How LoRA Works</h2>
                                <p>The key insight behind LoRA is that weight updates during fine-tuning have low
                                    intrinsic rank. Instead of updating a full weight matrix W (dd), LoRA decomposes
                                    the update into two smaller matrices: W = BA, where B is (dr) and A is (rd), with
                                    rank r much smaller than d (typically 8-64). During inference, the LoRA weights
                                    merge with the base model weights for zero additional latency.</p>
                                <pre><code># LoRA configuration with PEFT
from peft import LoraConfig, get_peft_model, TaskType

lora_config = LoraConfig(
    r=16,                    # Rank - higher = more capacity but more VRAM
    lora_alpha=32,           # Scaling factor (alpha/r = scaling)
    target_modules=[         # Which layers to adapt
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj"
    ],
    lora_dropout=0.05,
    bias="none",
    task_type=TaskType.CAUSAL_LM,
)

model = get_peft_model(base_model, lora_config)
model.print_trainable_parameters()
# trainable params: 41,943,040 || all params: 6,738,415,616 || trainable%: 0.62%</code></pre>

                                <h2>QLoRA: 4-bit Quantization + LoRA</h2>
                                <p>QLoRA takes this further by quantizing the base model to 4-bit precision (NF4/FP4)
                                    while keeping the LoRA adapters in full precision. This lets you fine-tune a 7B
                                    model on 6GB VRAM, or a 70B model on a single 48GB GPU. The quantization uses a
                                    novel NormalFloat4 data type that distributes quantization levels according to a
                                    normal distribution, preserving more information than uniform quantization.</p>
                                <pre><code>from transformers import BitsAndBytesConfig
import torch

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,  # Nested quantization
)

model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-3.1-8B",
    quantization_config=bnb_config,
    device_map="auto",
)</code></pre>

                                <h2>Dataset Preparation</h2>
                                <p>Data quality is everything. The standard format for instruction fine-tuning is the
                                    Alpaca format: instruction, input (optional), and output. For chat models, use the
                                    ChatML or Llama chat template. Always clean your data: remove duplicates, fix
                                    formatting, validate that outputs actually answer the instructions correctly.</p>
                                <pre><code># Example dataset format (JSON Lines)
{"instruction": "Explain what a Docker volume is",
 "input": "",
 "output": "A Docker volume is a persistent data storage mechanism..."}

# For chat format
{"conversations": [
    {"role": "system", "content": "You are a Linux expert."},
    {"role": "user", "content": "How do I check disk usage?"},
    {"role": "assistant", "content": "Use `df -h` for filesystem usage..."}
]}</code></pre>

                                <h2>Training with Unsloth</h2>
                                <p>Unsloth is my go-to library for fast fine-tuning. It provides 2x speed and 60% less
                                    VRAM compared to standard Hugging Face training by using optimized kernels. The API
                                    is simple and integrates with the HF ecosystem.</p>
                                <pre><code>from unsloth import FastLanguageModel
from trl import SFTTrainer
from transformers import TrainingArguments

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/Llama-3.1-8B-bnb-4bit",
    max_seq_length=4096,
    load_in_4bit=True,
)

model = FastLanguageModel.get_peft_model(
    model, r=16, lora_alpha=16,
    target_modules=["q_proj","k_proj","v_proj","o_proj",
                     "gate_proj","up_proj","down_proj"],
)

trainer = SFTTrainer(
    model=model,
    train_dataset=dataset,
    args=TrainingArguments(
        per_device_train_batch_size=2,
        gradient_accumulation_steps=4,
        num_train_epochs=3,
        learning_rate=2e-4,
        fp16=not torch.cuda.is_bf16_supported(),
        bf16=torch.cuda.is_bf16_supported(),
        warmup_steps=50,
        output_dir="./output",
    ),
)
trainer.train()</code></pre>

                                <h2>Evaluation and Merging</h2>
                                <p>After training, evaluate on a held-out test set. Check for overfitting by comparing
                                    train and eval loss. Common metrics include perplexity for language modeling
                                    quality, and task-specific metrics (BLEU, ROUGE, exact match) for downstream tasks.
                                    Once satisfied, merge the LoRA weights back into the base model for deployment:
                                    <code>model.merge_and_unload()</code>. Export to GGUF format for Ollama/llama.cpp
                                    inference.
                                </p>
                            </div>
                        </div>
                        <button class="blog-read-more"><span>Read More</span> <i
                                class="fas fa-chevron-down"></i></button>
                    </div>

                    <!-- AI Article 3: Transformer Architecture -->
                    <div class="blog-card reveal">
                        <div class="blog-card-header">
                            <h3>Transformer Architecture Deep Dive: Attention Is All You Need</h3>
                            <span class="tag"
                                style="background:rgba(168,85,247,0.1);color:#c084fc;border-color:rgba(168,85,247,0.2);">AI/ML</span>
                        </div>
                        <p>Understanding the transformer architecture from the ground up, with mathematical intuition
                            and implementation details.</p>
                        <div class="blog-excerpt">
                            <div class="blog-content">
                                <p>The transformer architecture, introduced in the 2017 paper "Attention Is All You
                                    Need," revolutionized deep learning. Every major AI model today (GPT, LLaMA, BERT,
                                    Stable Diffusion, Whisper) is built on transformers. Understanding how they work is
                                    essential for anyone working in AI/ML.</p>

                                <h2>Self-Attention Mechanism</h2>
                                <p>The core innovation is self-attention, which allows every token in a sequence to
                                    attend to every other token. For each token, we compute three vectors: Query (Q),
                                    Key (K), and Value (V) by multiplying the input embedding with learned weight
                                    matrices. The attention score between tokens is the dot product of Q and K, scaled
                                    by sqrt(d_k) to prevent gradient vanishing, then softmaxed to get weights, which are
                                    applied to V.</p>
                                <pre><code>import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class SelfAttention(nn.Module):
    def __init__(self, d_model, n_heads):
        super().__init__()
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads

        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)

    def forward(self, x, mask=None):
        B, T, C = x.shape

        Q = self.W_q(x).view(B, T, self.n_heads, self.d_k).transpose(1, 2)
        K = self.W_k(x).view(B, T, self.n_heads, self.d_k).transpose(1, 2)
        V = self.W_v(x).view(B, T, self.n_heads, self.d_k).transpose(1, 2)

        # Scaled dot-product attention
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)
        if mask is not None:
            scores = scores.masked_fill(mask == 0, float('-inf'))
        attn = F.softmax(scores, dim=-1)

        out = torch.matmul(attn, V)
        out = out.transpose(1, 2).contiguous().view(B, T, C)
        return self.W_o(out)</code></pre>

                                <h2>Multi-Head Attention</h2>
                                <p>Multi-head attention runs multiple attention computations in parallel with different
                                    learned projections, then concatenates the results. This allows the model to attend
                                    to different types of relationships simultaneously: one head might capture syntactic
                                    structure while another handles semantic meaning. GPT-3 uses 96 heads with
                                    d_model=12288.</p>

                                <h2>Feed-Forward Network</h2>
                                <p>After attention, each token passes through a position-wise feed-forward network
                                    (FFN), typically two linear layers with a non-linearity in between. Modern
                                    transformers (LLaMA, Mistral) use SwiGLU activation instead of ReLU, which
                                    consistently improves performance.</p>
                                <pre><code>class FeedForward(nn.Module):
    def __init__(self, d_model, d_ff):
        super().__init__()
        self.gate = nn.Linear(d_model, d_ff, bias=False)
        self.up = nn.Linear(d_model, d_ff, bias=False)
        self.down = nn.Linear(d_ff, d_model, bias=False)

    def forward(self, x):
        # SwiGLU activation
        return self.down(F.silu(self.gate(x)) * self.up(x))</code></pre>

                                <h2>Positional Encoding</h2>
                                <p>Transformers have no inherent notion of position, so positional information must be
                                    injected. The original paper used sinusoidal encodings, but modern models use Rotary
                                    Position Embedding (RoPE), which encodes position through rotation matrices applied
                                    to Q and K vectors. RoPE enables length extrapolation (training on 4K context,
                                    inferring on 32K) when combined with techniques like NTK-aware scaling or YaRN.</p>

                                <h2>Modern Improvements</h2>
                                <p>Key architectural improvements since the original transformer include: RMSNorm
                                    instead of LayerNorm (faster, equally effective), Grouped-Query Attention (GQA)
                                    which shares K/V heads to reduce memory during inference, Flash Attention for
                                    IO-aware exact attention computation (2-4x speedup), KV-cache for autoregressive
                                    generation, and sliding window attention (Mistral) for efficient long-context
                                    handling. Understanding these building blocks lets you read any modern LLM paper and
                                    understand what is happening under the hood.</p>
                            </div>
                        </div>
                        <button class="blog-read-more"><span>Read More</span> <i
                                class="fas fa-chevron-down"></i></button>
                    </div>

                    <!-- AI Article 4: LLM Agents -->
                    <div class="blog-card reveal">
                        <div class="blog-card-header">
                            <h3>Building LLM Agents: Tool Use, Planning, and Memory</h3>
                            <span class="tag"
                                style="background:rgba(168,85,247,0.1);color:#c084fc;border-color:rgba(168,85,247,0.2);">AI/ML</span>
                        </div>
                        <p>How to build AI agents that can use tools, reason about complex tasks, and maintain
                            conversation memory.</p>
                        <div class="blog-excerpt">
                            <div class="blog-content">
                                <p>LLM agents go beyond simple question-answering by giving models the ability to take
                                    actions, use external tools, and reason through multi-step problems. The agent
                                    paradigm is arguably the most important evolution in practical AI since ChatGPT
                                    launched.</p>

                                <h2>ReAct Pattern: Reasoning + Acting</h2>
                                <p>The ReAct (Reasoning and Acting) pattern interleaves thought and action steps. The
                                    LLM generates a thought about what to do next, takes an action (calls a tool),
                                    observes the result, and repeats until the task is complete. This mirrors how humans
                                    solve problems: think, try, observe, adjust.</p>
                                <pre><code>SYSTEM_PROMPT = """You are an AI assistant with access to tools.
For each step, respond with:
Thought: [your reasoning]
Action: [tool_name(args)]
Observation: [tool result - filled by system]
... repeat as needed ...
Final Answer: [your response]

Available tools:
- search(query) - Search the web
- calculate(expression) - Evaluate math
- read_file(path) - Read file contents
- run_code(code) - Execute Python code
"""

def agent_loop(query, max_steps=10):
    messages = [{"role": "system", "content": SYSTEM_PROMPT}]
    messages.append({"role": "user", "content": query})

    for step in range(max_steps):
        response = llm.chat(messages)
        text = response.content

        if "Final Answer:" in text:
            return text.split("Final Answer:")[-1].strip()

        action = parse_action(text)
        if action:
            result = execute_tool(action.tool, action.args)
            messages.append({"role": "assistant", "content": text})
            messages.append({"role": "user",
                           "content": f"Observation: {result}"})

    return "Max steps reached without resolution."</code></pre>

                                <h2>Function Calling</h2>
                                <p>Modern LLMs support structured function calling where tools are described as JSON
                                    schemas. The model outputs a structured tool call (function name + arguments) rather
                                    than free-form text. This is more reliable than parsing text-based actions. OpenAI,
                                    Anthropic, and open-source models like Hermes and Functionary all support this.</p>
                                <pre><code>tools = [{
    "type": "function",
    "function": {
        "name": "get_weather",
        "description": "Get current weather for a location",
        "parameters": {
            "type": "object",
            "properties": {
                "location": {"type": "string", "description": "City name"},
                "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]}
            },
            "required": ["location"]
        }
    }
}]

response = client.chat.completions.create(
    model="gpt-4o",
    messages=messages,
    tools=tools,
    tool_choice="auto"
)</code></pre>

                                <h2>Memory Systems</h2>
                                <p>Agents need memory to handle long conversations and learn from past interactions.
                                    Short-term memory uses the conversation context window. For long-term memory,
                                    implement a vector store of past conversation summaries, key facts, and user
                                    preferences. At each turn, retrieve relevant memories and inject them into the
                                    system prompt. Some architectures use an explicit scratchpad where the agent can
                                    write and read structured notes.</p>

                                <h2>Planning and Decomposition</h2>
                                <p>For complex tasks, agents benefit from planning before acting. The Plan-and-Execute
                                    pattern first generates a high-level plan (break the task into subtasks), then
                                    executes each subtask sequentially, revising the plan based on intermediate results.
                                    This avoids the common failure mode where agents get lost in a chain of tool calls
                                    without a clear direction. Frameworks like LangGraph and CrewAI provide structured
                                    ways to build multi-agent systems where different agents specialize in different
                                    subtasks.</p>
                            </div>
                        </div>
                        <button class="blog-read-more"><span>Read More</span> <i
                                class="fas fa-chevron-down"></i></button>
                    </div>

                    <!-- AI Article 5: Computer Vision with PyTorch -->
                    <div class="blog-card reveal">
                        <div class="blog-card-header">
                            <h3>Computer Vision with PyTorch: From CNNs to Vision Transformers</h3>
                            <span class="tag"
                                style="background:rgba(168,85,247,0.1);color:#c084fc;border-color:rgba(168,85,247,0.2);">AI/ML</span>
                        </div>
                        <p>Building image classification, object detection, and segmentation models with modern PyTorch
                            techniques.</p>
                        <div class="blog-excerpt">
                            <div class="blog-content">
                                <p>Computer vision has undergone a revolution: from hand-crafted features to CNNs to
                                    Vision Transformers. Understanding the full evolution helps you choose the right
                                    approach for your specific task and dataset size.</p>

                                <h2>CNN Foundations</h2>
                                <p>Convolutional Neural Networks learn hierarchical spatial features through learnable
                                    filters. Early layers detect edges and textures, middle layers combine these into
                                    parts (eyes, wheels), and deep layers recognize full objects. The key components
                                    are: convolutional layers (feature extraction), pooling layers (spatial
                                    downsampling), and fully connected layers (classification).</p>
                                <pre><code>import torch
import torch.nn as nn
import torchvision.models as models
from torchvision import transforms

# Modern approach: use pretrained backbone + custom head
class ImageClassifier(nn.Module):
    def __init__(self, num_classes, pretrained=True):
        super().__init__()
        # EfficientNet-B0 as backbone
        self.backbone = models.efficientnet_b0(
            weights=models.EfficientNet_B0_Weights.DEFAULT if pretrained else None
        )
        # Replace classifier head
        in_features = self.backbone.classifier[1].in_features
        self.backbone.classifier = nn.Sequential(
            nn.Dropout(p=0.3),
            nn.Linear(in_features, num_classes)
        )

    def forward(self, x):
        return self.backbone(x)

# Data augmentation pipeline
train_transform = transforms.Compose([
    transforms.RandomResizedCrop(224),
    transforms.RandomHorizontalFlip(),
    transforms.ColorJitter(brightness=0.2, contrast=0.2),
    transforms.RandomRotation(15),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])</code></pre>

                                <h2>Training Loop with Mixed Precision</h2>
                                <p>Modern training uses mixed precision (FP16/BF16) for 2x speedup and lower memory
                                    usage, gradient accumulation for effective larger batch sizes, and learning rate
                                    scheduling with warmup.</p>
                                <pre><code>from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()
optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.01)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)

for epoch in range(num_epochs):
    model.train()
    for images, labels in train_loader:
        images, labels = images.cuda(), labels.cuda()

        with autocast():
            outputs = model(images)
            loss = F.cross_entropy(outputs, labels)

        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
        optimizer.zero_grad()

    scheduler.step()</code></pre>

                                <h2>Vision Transformers (ViT)</h2>
                                <p>ViT treats an image as a sequence of patches (e.g., 1616 pixels each), flattens them
                                    into vectors, adds positional embeddings, and processes them with a standard
                                    transformer encoder. For small datasets, ViT underperforms CNNs because it lacks the
                                    inductive bias of convolutions. But with large datasets or pretrained weights, ViT
                                    matches or exceeds CNN performance. Hybrid approaches like ConvNeXt combine CNN
                                    efficiency with transformer-inspired design.</p>

                                <h2>Object Detection with YOLO</h2>
                                <p>For real-time object detection, the YOLO (You Only Look Once) family remains the
                                    practical choice. YOLOv8/v11 from Ultralytics provides state-of-the-art detection,
                                    segmentation, and pose estimation with a simple API. Training a custom detector
                                    requires annotated bounding boxes (use tools like Label Studio or Roboflow) and
                                    typically converges in 50-100 epochs on datasets of 500+ images per class.</p>
                            </div>
                        </div>
                        <button class="blog-read-more"><span>Read More</span> <i
                                class="fas fa-chevron-down"></i></button>
                    </div>

                    <!-- AI Article 6: AI Ethics and Safety -->
                    <div class="blog-card reveal">
                        <div class="blog-card-header">
                            <h3>AI Safety and Alignment: Technical Challenges and Practical Solutions</h3>
                            <span class="tag"
                                style="background:rgba(168,85,247,0.1);color:#c084fc;border-color:rgba(168,85,247,0.2);">AI/ML</span>
                        </div>
                        <p>Understanding prompt injection, jailbreaking, model alignment techniques, and building safer
                            AI systems.</p>
                        <div class="blog-excerpt">
                            <div class="blog-content">
                                <p>As AI systems become more powerful and widely deployed, safety and alignment become
                                    critical engineering challenges. This is not just about ethics; it is about building
                                    systems that reliably do what we want and do not cause harm.</p>

                                <h2>Prompt Injection Attacks</h2>
                                <p>Prompt injection is the SQL injection of the AI era. An attacker embeds malicious
                                    instructions in user input that override the system prompt. Direct injection
                                    explicitly tells the model to ignore its instructions. Indirect injection hides
                                    instructions in external content (web pages, documents) that the model processes.
                                </p>
                                <pre><code># Vulnerable pattern
user_input = request.form["query"]
response = llm.chat(f"You are a helpful assistant. User: {user_input}")

# Defense: structured messages + input sanitization
def safe_query(user_input):
    # Sanitize input
    cleaned = sanitize_for_llm(user_input)

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": cleaned}
        ],
        # Use structured outputs to constrain response format
        response_format={"type": "json_schema", "json_schema": schema}
    )
    return validate_output(response)</code></pre>

                                <h2>RLHF: Reinforcement Learning from Human Feedback</h2>
                                <p>RLHF is the primary technique for aligning LLMs with human preferences. The process
                                    has three stages: (1) supervised fine-tuning on high-quality demonstrations, (2)
                                    training a reward model on human preference comparisons (which response is better?),
                                    (3) optimizing the LLM policy using PPO (Proximal Policy Optimization) to maximize
                                    the reward while staying close to the original model via KL divergence penalty.</p>
                                <p>DPO (Direct Preference Optimization) simplifies this by eliminating the separate
                                    reward model. It directly trains the language model on preference pairs, treating
                                    the language model itself as the reward model. DPO is simpler, more stable, and
                                    produces comparable results to RLHF with PPO.</p>

                                <h2>Guardrails and Output Filtering</h2>
                                <p>Defense in depth is essential. Implement multiple layers: input classifiers that
                                    detect malicious prompts, output filters that check for harmful content, structured
                                    outputs that constrain the response format, and monitoring systems that flag
                                    anomalous model behavior. Libraries like NeMo Guardrails and Microsoft's Guidance
                                    provide frameworks for building these safety layers.</p>
                                <pre><code># Simple output guardrail example
import re

BLOCKED_PATTERNS = [
    r"(?i)(how to make|instructions for).*(weapon|explosive|drug)",
    r"(?i)personal.*(address|phone|ssn|credit card)",
]

def check_output(response_text):
    for pattern in BLOCKED_PATTERNS:
        if re.search(pattern, response_text):
            return "I cannot provide that information."
    return response_text</code></pre>

                                <h2>Red Teaming and Evaluation</h2>
                                <p>Systematic red teaming tests model safety across categories: harmful content
                                    generation, bias and stereotyping, privacy violations, and instruction following
                                    failures. Build an evaluation dataset covering these categories and run it against
                                    every model update. Automated red teaming using adversarial LLMs can scale testing,
                                    but human evaluation remains essential for subtle failure modes. The key principle:
                                    safety is not a feature you add at the end, it must be integrated into every stage
                                    of the development pipeline.</p>
                            </div>
                        </div>
                        <button class="blog-read-more"><span>Read More</span> <i
                                class="fas fa-chevron-down"></i></button>
                    </div>

                    <!-- Mobile Article 1: React Native -->
                    <div class="blog-card reveal">
                        <div class="blog-card-header">
                            <h3>React Native Internals: New Architecture and Performance Optimization</h3>
                            <span class="tag"
                                style="background:rgba(34,197,94,0.1);color:#4ade80;border-color:rgba(34,197,94,0.2);">Mobile</span>
                        </div>
                        <p>Deep dive into React Native's new architecture (Fabric, TurboModules, JSI) and practical
                            performance techniques.</p>
                        <div class="blog-excerpt">
                            <div class="blog-content">
                                <p>React Native's new architecture fundamentally changes how JavaScript communicates
                                    with native code. The old bridge-based architecture serialized all data as JSON and
                                    sent it asynchronously across a bridge, creating bottlenecks. The new architecture
                                    replaces this with direct, synchronous communication.</p>

                                <h2>JSI: JavaScript Interface</h2>
                                <p>JSI (JavaScript Interface) allows JavaScript to hold direct references to C++ objects
                                    and call native methods synchronously without serialization. This is the foundation
                                    that enables everything else in the new architecture.</p>
                                <pre><code>// Old Bridge approach (async, serialized JSON)
NativeModules.MyModule.getData(callback);

// New JSI approach (sync, direct reference)
// C++ host object exposed to JS
const result = global.__myModule.getDataSync();

// TurboModule definition (TypeScript spec)
import { TurboModuleRegistry } from 'react-native';

export interface Spec extends TurboModule {
  getConstants(): {| version: string |};
  multiply(a: number, b: number): Promise&lt;number&gt;;
  getDeviceName(): string; // Synchronous!
}

export default TurboModuleRegistry.getEnforcing&lt;Spec&gt;('Calculator');</code></pre>

                                <h2>Fabric Renderer</h2>
                                <p>Fabric is the new rendering system. It creates a C++ shadow tree representation of
                                    the UI, enabling synchronous layout measurement, concurrent rendering (React 18
                                    features), and direct manipulation of native views from JavaScript. Layout
                                    calculations using Yoga (flexbox engine) happen in C++ instead of going through the
                                    bridge.</p>

                                <h2>Performance Optimization</h2>
                                <p>Practical techniques for React Native performance: use <code>FlashList</code> instead
                                    of FlatList for lists (up to 5x faster). Memoize expensive components with
                                    <code>React.memo()</code> and <code>useMemo()</code>. Use
                                    <code>react-native-reanimated</code> for animations that run on the UI thread. Avoid
                                    inline function definitions in render methods as they cause unnecessary re-renders.
                                </p>
                                <pre><code>import { FlashList } from "@shopify/flash-list";
import Animated, { useSharedValue, useAnimatedStyle,
    withSpring } from 'react-native-reanimated';

// Optimized list with FlashList
const OptimizedList = React.memo(({ data }) =&gt; (
  &lt;FlashList
    data={data}
    renderItem={renderItem}
    estimatedItemSize={80}
    keyExtractor={keyExtractor}
    removeClippedSubviews={true}
  /&gt;
));

// UI-thread animation with Reanimated
function AnimatedCard() {
  const scale = useSharedValue(1);
  const style = useAnimatedStyle(() =&gt; ({
    transform: [{ scale: withSpring(scale.value) }],
  }));

  return (
    &lt;Pressable onPressIn={() =&gt; (scale.value = 0.95)}
               onPressOut={() =&gt; (scale.value = 1)}&gt;
      &lt;Animated.View style={[styles.card, style]} /&gt;
    &lt;/Pressable&gt;
  );
}</code></pre>

                                <h2>Hermes Engine</h2>
                                <p>Hermes is React Native's custom JavaScript engine optimized for mobile. It compiles
                                    JavaScript to bytecode at build time (AOT compilation), resulting in faster startup
                                    times, lower memory usage, and smaller app sizes compared to JavaScriptCore. Hermes
                                    supports modern JavaScript features and is now the default engine for React Native.
                                    Profile performance using the Hermes profiler in Flipper to identify bottleneck
                                    functions and optimize them.</p>
                            </div>
                        </div>
                        <button class="blog-read-more"><span>Read More</span> <i
                                class="fas fa-chevron-down"></i></button>
                    </div>

                    <!-- Mobile Article 2: Flutter State Management -->
                    <div class="blog-card reveal">
                        <div class="blog-card-header">
                            <h3>Flutter State Management: BLoC, Riverpod, and Practical Patterns</h3>
                            <span class="tag"
                                style="background:rgba(34,197,94,0.1);color:#4ade80;border-color:rgba(34,197,94,0.2);">Mobile</span>
                        </div>
                        <p>Comparing Flutter state management solutions with real-world examples, architecture patterns,
                            and testing strategies.</p>
                        <div class="blog-excerpt">
                            <div class="blog-content">
                                <p>State management in Flutter is the most debated topic in the ecosystem. There are too
                                    many options and no single "right" answer. After years of building production
                                    Flutter apps, here is my practical guide to the major solutions and when to use each
                                    one.</p>

                                <h2>BLoC Pattern (Business Logic Component)</h2>
                                <p>BLoC separates business logic from UI using streams. Events go in, states come out.
                                    It is verbose but extremely testable and predictable. Best for large teams and
                                    complex business logic where explicit state transitions matter.</p>
                                <pre><code>// Event definitions
abstract class AuthEvent {}
class LoginRequested extends AuthEvent {
  final String email;
  final String password;
  LoginRequested(this.email, this.password);
}
class LogoutRequested extends AuthEvent {}

// State definitions
abstract class AuthState {}
class AuthInitial extends AuthState {}
class AuthLoading extends AuthState {}
class AuthSuccess extends AuthState {
  final User user;
  AuthSuccess(this.user);
}
class AuthFailure extends AuthState {
  final String error;
  AuthFailure(this.error);
}

// BLoC implementation
class AuthBloc extends Bloc&lt;AuthEvent, AuthState&gt; {
  final AuthRepository _repo;

  AuthBloc(this._repo) : super(AuthInitial()) {
    on&lt;LoginRequested&gt;(_onLogin);
    on&lt;LogoutRequested&gt;(_onLogout);
  }

  Future&lt;void&gt; _onLogin(LoginRequested event, Emitter&lt;AuthState&gt; emit) async {
    emit(AuthLoading());
    try {
      final user = await _repo.login(event.email, event.password);
      emit(AuthSuccess(user));
    } catch (e) {
      emit(AuthFailure(e.toString()));
    }
  }
}</code></pre>

                                <h2>Riverpod</h2>
                                <p>Riverpod is the evolution of Provider, fixing its fundamental limitations. It is
                                    compile-safe (no runtime errors from missing providers), supports auto-disposal, and
                                    works without BuildContext. I consider it the best general-purpose solution for most
                                    Flutter apps.</p>
                                <pre><code>// Riverpod providers
@riverpod
class AuthNotifier extends _$AuthNotifier {
  @override
  AsyncValue&lt;User?&gt; build() =&gt; const AsyncValue.data(null);

  Future&lt;void&gt; login(String email, String password) async {
    state = const AsyncValue.loading();
    state = await AsyncValue.guard(() async {
      final repo = ref.read(authRepositoryProvider);
      return await repo.login(email, password);
    });
  }
}

// Usage in widget
class LoginScreen extends ConsumerWidget {
  @override
  Widget build(BuildContext context, WidgetRef ref) {
    final authState = ref.watch(authNotifierProvider);

    return authState.when(
      data: (user) =&gt; user != null ? HomeScreen() : LoginForm(),
      loading: () =&gt; CircularProgressIndicator(),
      error: (e, st) =&gt; ErrorWidget(e.toString()),
    );
  }
}</code></pre>

                                <h2>Architecture Patterns</h2>
                                <p>Regardless of state management choice, follow clean architecture: separate your code
                                    into layers. The data layer handles API calls and local storage. The domain layer
                                    contains business logic and models. The presentation layer holds widgets and state
                                    management. This separation makes testing easy: mock the repository, test the
                                    BLoC/notifier in isolation, and widget-test the UI.</p>
                                <pre><code>// Testing BLoC with bloc_test
blocTest&lt;AuthBloc, AuthState&gt;(
  'emits [loading, success] on valid login',
  build: () {
    when(() =&gt; mockRepo.login(any(), any()))
        .thenAnswer((_) async =&gt; testUser);
    return AuthBloc(mockRepo);
  },
  act: (bloc) =&gt; bloc.add(LoginRequested('test@mail.com', 'pass')),
  expect: () =&gt; [AuthLoading(), AuthSuccess(testUser)],
);</code></pre>

                                <h2>When to Use What</h2>
                                <p>Use <strong>setState</strong> for simple, local UI state (toggle, form input). Use
                                    <strong>Riverpod</strong> for most apps: it scales well, is easy to learn, and has
                                    excellent DX. Use <strong>BLoC</strong> for large enterprise apps where explicit
                                    event/state mapping and strict separation of concerns are required. Avoid using
                                    multiple state management solutions in the same app; pick one and be consistent.
                                </p>
                            </div>
                        </div>
                        <button class="blog-read-more"><span>Read More</span> <i
                                class="fas fa-chevron-down"></i></button>
                    </div>

                    <!-- Web Article 1: WebSocket -->
                    <div class="blog-card reveal">
                        <div class="blog-card-header">
                            <h3>Real-Time Web Applications: WebSockets, SSE, and Socket.IO</h3>
                            <span class="tag"
                                style="background:rgba(34,197,94,0.1);color:#4ade80;border-color:rgba(34,197,94,0.2);">Web</span>
                        </div>
                        <p>Building real-time features with WebSockets, handling scaling challenges, and choosing the
                            right transport protocol.</p>
                        <div class="blog-excerpt">
                            <div class="blog-content">
                                <p>Real-time communication is essential for modern web apps: chat, notifications,
                                    collaborative editing, live dashboards, gaming. HTTP was designed for
                                    request-response, not persistent connections. WebSockets, Server-Sent Events (SSE),
                                    and libraries like Socket.IO bridge this gap.</p>

                                <h2>WebSocket Server with Node.js</h2>
                                <p>WebSockets provide full-duplex communication over a single TCP connection. After an
                                    initial HTTP handshake upgrade, both client and server can send messages at any time
                                    with minimal overhead (2-6 bytes per frame vs ~800 bytes for HTTP headers).</p>
                                <pre><code>import { WebSocketServer } from 'ws';
import { createServer } from 'http';

const server = createServer();
const wss = new WebSocketServer({ server });

// Connection handling with rooms
const rooms = new Map();

wss.on('connection', (ws, req) =&gt; {
  const userId = authenticateFromHeaders(req.headers);
  ws.userId = userId;
  ws.isAlive = true;

  ws.on('pong', () =&gt; { ws.isAlive = true; });

  ws.on('message', (data) =&gt; {
    const msg = JSON.parse(data);

    switch (msg.type) {
      case 'join_room':
        joinRoom(ws, msg.roomId);
        break;
      case 'message':
        broadcastToRoom(msg.roomId, {
          type: 'message',
          userId,
          content: msg.content,
          timestamp: Date.now()
        });
        break;
    }
  });

  ws.on('close', () =&gt; removeFromAllRooms(ws));
});

function broadcastToRoom(roomId, data) {
  const members = rooms.get(roomId) || new Set();
  const payload = JSON.stringify(data);
  for (const client of members) {
    if (client.readyState === 1) client.send(payload);
  }
}

// Heartbeat to detect dead connections
setInterval(() =&gt; {
  wss.clients.forEach(ws =&gt; {
    if (!ws.isAlive) return ws.terminate();
    ws.isAlive = false;
    ws.ping();
  });
}, 30000);</code></pre>

                                <h2>Server-Sent Events (SSE)</h2>
                                <p>SSE is simpler than WebSockets for one-directional server-to-client streaming. It
                                    uses standard HTTP, supports automatic reconnection, and works through proxies
                                    without special configuration. Perfect for live feeds, notifications, and AI
                                    streaming responses.</p>
                                <pre><code>// SSE endpoint
app.get('/events', (req, res) =&gt; {
  res.writeHead(200, {
    'Content-Type': 'text/event-stream',
    'Cache-Control': 'no-cache',
    'Connection': 'keep-alive',
  });

  const sendEvent = (data, event = 'message') =&gt; {
    res.write(`event: ${event}\n`);
    res.write(`data: ${JSON.stringify(data)}\n\n`);
  };

  // Send periodic updates
  const interval = setInterval(() =&gt; {
    sendEvent({ cpu: getCpuUsage(), memory: getMemUsage() }, 'metrics');
  }, 1000);

  req.on('close', () =&gt; clearInterval(interval));
});

// Client
const source = new EventSource('/events');
source.addEventListener('metrics', (e) =&gt; {
  const data = JSON.parse(e.data);
  updateDashboard(data);
});</code></pre>

                                <h2>Scaling WebSockets</h2>
                                <p>Single-server WebSockets are simple. Scaling horizontally is where complexity
                                    appears. When you have multiple server instances behind a load balancer, clients
                                    connected to different servers cannot communicate directly. Solutions include: Redis
                                    Pub/Sub as a message broker between servers, sticky sessions to keep clients on the
                                    same server, or dedicated message buses like NATS or RabbitMQ. Socket.IO has
                                    built-in Redis adapter support for multi-server deployments.</p>

                                <h2>WebSocket vs SSE vs Polling</h2>
                                <p>Use <strong>WebSockets</strong> when you need bidirectional real-time communication
                                    (chat, gaming, collaborative editing). Use <strong>SSE</strong> for server-to-client
                                    streaming (notifications, live feeds, AI responses). Use <strong>long
                                        polling</strong> only as a fallback when WebSockets and SSE are blocked. Never
                                    use short polling for real-time features; it wastes bandwidth and battery.</p>
                            </div>
                        </div>
                        <button class="blog-read-more"><span>Read More</span> <i
                                class="fas fa-chevron-down"></i></button>
                    </div>

                    <!-- Web Article 2: GraphQL -->
                    <div class="blog-card reveal">
                        <div class="blog-card-header">
                            <h3>GraphQL Schema Design: Types, Resolvers, and Performance</h3>
                            <span class="tag"
                                style="background:rgba(34,197,94,0.1);color:#4ade80;border-color:rgba(34,197,94,0.2);">Web</span>
                        </div>
                        <p>Designing efficient GraphQL APIs with proper schema design, resolver patterns, DataLoader,
                            and security considerations.</p>
                        <div class="blog-excerpt">
                            <div class="blog-content">
                                <p>GraphQL solves real problems that REST APIs have: over-fetching (getting data you do
                                    not need), under-fetching (needing multiple requests), and API versioning
                                    complexity. But it introduces its own challenges around performance, security, and
                                    complexity. Understanding these tradeoffs is essential for building production
                                    GraphQL APIs.</p>

                                <h2>Schema Design Principles</h2>
                                <p>Your schema is your API contract. Design it from the client's perspective, not your
                                    database schema. Use meaningful types, avoid generic "data" fields, and leverage
                                    GraphQL's type system for validation.</p>
                                <pre><code># Schema definition
type User {
  id: ID!
  username: String!
  email: String!
  avatar: String
  posts(first: Int = 10, after: String): PostConnection!
  createdAt: DateTime!
}

type Post {
  id: ID!
  title: String!
  content: String!
  author: User!
  tags: [Tag!]!
  comments(first: Int = 20): CommentConnection!
  likeCount: Int!
  createdAt: DateTime!
}

# Relay-style pagination
type PostConnection {
  edges: [PostEdge!]!
  pageInfo: PageInfo!
  totalCount: Int!
}

type PostEdge {
  node: Post!
  cursor: String!
}

type Query {
  user(id: ID!): User
  posts(filter: PostFilter, first: Int, after: String): PostConnection!
  searchPosts(query: String!): [Post!]!
}

type Mutation {
  createPost(input: CreatePostInput!): Post!
  updatePost(id: ID!, input: UpdatePostInput!): Post!
  deletePost(id: ID!): Boolean!
}

input CreatePostInput {
  title: String!
  content: String!
  tagIds: [ID!]
}</code></pre>

                                <h2>Resolvers and DataLoader</h2>
                                <p>The N+1 problem is GraphQL's biggest performance trap. If you query 50 posts with
                                    their authors, a naive implementation makes 1 query for posts + 50 queries for
                                    authors. DataLoader batches and caches these lookups.</p>
                                <pre><code>import DataLoader from 'dataloader';

// Create DataLoader per request (important for caching isolation)
function createLoaders(db) {
  return {
    userLoader: new DataLoader(async (userIds) =&gt; {
      const users = await db.users.findMany({
        where: { id: { in: userIds } }
      });
      // Return in same order as input IDs
      const userMap = new Map(users.map(u =&gt; [u.id, u]));
      return userIds.map(id =&gt; userMap.get(id) || null);
    }),
  };
}

// Resolver using DataLoader
const resolvers = {
  Post: {
    author: (post, _, { loaders }) =&gt; {
      return loaders.userLoader.load(post.authorId);
      // Automatically batched! 50 calls become 1 SQL query
    },
  },
  Query: {
    posts: async (_, { filter, first, after }, { db }) =&gt; {
      const cursor = after ? decodeCursor(after) : null;
      const posts = await db.posts.findMany({
        where: buildFilter(filter),
        take: first + 1, // Fetch one extra to check hasNextPage
        cursor: cursor ? { id: cursor } : undefined,
        orderBy: { createdAt: 'desc' },
      });

      const hasNextPage = posts.length &gt; first;
      const edges = posts.slice(0, first).map(post =&gt; ({
        node: post,
        cursor: encodeCursor(post.id),
      }));

      return { edges, pageInfo: { hasNextPage, endCursor: edges.at(-1)?.cursor } };
    },
  },
};</code></pre>

                                <h2>Security Considerations</h2>
                                <p>GraphQL APIs are vulnerable to unique attacks: query depth attacks (deeply nested
                                    queries that overwhelm the server), query width attacks (requesting every field on
                                    every type), and introspection leaking (exposing your entire schema in production).
                                    Implement: query depth limiting (max 10-15 levels), query complexity analysis with
                                    cost scoring, field-level authorization, rate limiting per query complexity rather
                                    than per request, and disable introspection in production.</p>
                                <pre><code>import depthLimit from 'graphql-depth-limit';
import { createComplexityLimitRule } from 'graphql-validation-complexity';

const server = new ApolloServer({
  schema,
  validationRules: [
    depthLimit(10),
    createComplexityLimitRule(1000, {
      scalarCost: 1,
      objectCost: 2,
      listFactor: 10,
    }),
  ],
  introspection: process.env.NODE_ENV !== 'production',
});</code></pre>

                                <h2>When to Use GraphQL vs REST</h2>
                                <p>Use GraphQL when: clients need flexible data fetching (mobile apps with varying
                                    screen sizes), you have multiple consuming clients with different data needs, or
                                    your data model has complex relationships. Stick with REST when: your API is simple
                                    CRUD, you need HTTP caching, you are building public APIs (simpler to document and
                                    consume), or your team is small and does not need the additional complexity. Many
                                    successful architectures use both: REST for simple endpoints, GraphQL for complex
                                    data aggregation.</p>
                            </div>
                        </div>
                        <button class="blog-read-more"><span>Read More</span> <i
                                class="fas fa-chevron-down"></i></button>
                    </div>

                </div>
            </div>
        </section>
    </div>

    <footer class="footer">
        <div class="container">
            <span class="footer-brand">root0emir</span>
            <p>2026 root0emir | All rights reserved</p>
        </div>
    </footer>
    <script src="static/script.js"></script>
</body>


</html>